Lecture 1: Introduction to Web Mining
Content
1. What is WWW?
2. What is data mining?
3. What is web mining?
1. What is WWW?
WWW (web) affects our daily life
Web changes way of information retrieval
Huge information, welknown, accessible & searchable
Billions linked web pages created by millions authors
Before, we ask friend/family for a book
With Internet, everything is simple with some clicks right at home or office
Web is an important transaction channel
We can buy almost everything without going to shop
We can easily connect to make friend, discuss, share with anyone in world
− Web is a virtual world reflecting real world
www definition
“Web is a computer network allowing a user from a computer to access information stored in a remote computer in network"
Web is based on client-server architecture
User uses a client to access data in a server
Web browsing is done by a browser (IE, Firefox, Chrome):
Send request to server
Get response from server
Compile HTML
Display graphical content
Web documents are hypertext allowing author to link their document to any document on internet via hyperlink
To view linked document, user only needs to click on hyperlink
Hypertext is invented by Ted Nelson in 1965
Hypertext allows to embed multimedia (image, video, voice)
Web history
Web is invented by Tim Berners-Lee (CERN) in 1989 in a proposal on a protocol for managing distributed documents:
Hierarchy structure shows limitations
Propose a protocol to request information stored in a remote computer
Propose a common document format allowing a document to link to other documents
Initial components of web:
Server
Browser
HTTP
HTML
URL
Web history (cont)
Netscape
IE
Dot com bubble
ARPANET
TCP/IP
Internet
Google
MSN
(Bing)
Web history (cont)
Mosaic was created in 1993 at Illinois University
First browser with graphical interface & mouse interaction
− Runs on UNIX, Macintosh & Windows
In 1994, Mosaic was publicly released as Netscape
- In 1995, Internet Explorer of Microsoft was released
Web history (cont)
ARPANET (1969) is developed by ARPA
- TCP/IP (1973) allows networks to connect
- Internet was born in 1982 based onTCP/IP
Web history (cont)
Information shared on Web raises need for efficient information retrieval
Excite search engine was introduced by Stanford in 1993
Yahoo! was found in 1994, providing information in hierarchical structure
Google was found in 1998
Microsoft introduces MSN in 2003 (Bing)
W3C ( World Wide Web Consortium) was found in 1994 by MIT & CERN
Leading development of Web
Building standards for Web
Setting up specifications & reference to support interaction between Web applications
WWW was first organized in 1994
1995 – 2001, Web is developed & expanded
2001: dotcom bubble
2. What is data mining?
2.1 Definition of DM
2.2 History of DM
2.3 Data types
2.4 Discoverable patterns
2.5 Techniques in DM
2.6 Applications of DM
2.7 Challenges in DM
2.1 Definition of DM
Known as Knowledge Discovery in Databases
- “the process of discovering useful patterns or knowledge from data sources"
- Patterns should be: correct, useful & understandable
- Data sources: DB, text, image, Web v.v.
- DM is an interdiscipline domain including machine learning, statistics, DB, artificial intelligence, information retrieval & visualization
- Main tasks in DM: supervised learning (classification), unsupervised learning (clustering), association rule mining, sequential mining
Definition of DM (cont)
Data analyst selects data sources & targets based on domain knowledge
Preprocessing:
Raw data is typical unsuitable for mining
Need cleansing to remove noise & abnormaly
When data is too large or contains irrelevant attribute, it requires sampling or feature/attribute selection
DM: Applying techniques on clean data to discover knowledge
Post processing: Choose useful pattern/knowledge using evaluation &/or visualization methods
 process repeats until satisfied
Traditional techniques are based on structured data. With development of Web, semi-structured & non-structured data becomes more important
2.2 History of DM
DBMS (70'-80')
Hierarchical DBMS
- Network DBMS
- Data modeling: entity – relation model
- Indexing & accessing
- Query language: SQL
- User interface, form, report
- Query processing & optimization
- Transaction, concurrency management, recovery
- OLTP
Advanced DBMS (80'-now)
Advanced data models: Extended relation model, object relation model
Complex data management: spatial, temporal, multimedia, sequence; structural objects, moving objects
Data streams & meta-physic data systems
Web DBs (XML, semantic web)
Uncertainty data management & data cleansing
Heterogenous source integration
Text DBMS & integration with information retrieval
Big data management
Tuning DBMS & flexible systems
Advanced query: ranking
Cloud computing & parallel data processing
Data policy & security
Advanced data analytics
(80'-now)
Data warehouse & OLAP
- DM & knowledge discovery: classification, clustering, abnomaly detection, association & correlation, summarization, comparision, pattern discovery, trend & variance analysis
- Complex data mining: stream, sequence, text, spatial, temporal, multimedia, web, network
- Applications of DM: business, social, commerce, bank, telecom, science & technology, social network
2.3 Data types
1. Cleansing
2. Integration
3. Selection
4. Transformation
5. Mining
6. Evaluation
7. Representation
Pattern
DB
Knowledge
Data warehouse
Phases in DM
Data from DB
DBMS includes a set of relation data called DB & programs to manage & access data.
 programs provide mechanisms to
A relational DB contains tables
Each table contains a set of attribute (column, field)
 records (row) in a table represents an object identified by a unique key & described by attributes
DBs are accessed using queries
Define DB structure & data storage
Describe & manage concurrency, share, distribute
Guarantee consistency & security
Queries are transformed into a set of relational operations like combination, selection & are then optimized
A query allow to retrieve a specific part of data
In relational DB mining, main tasks are detecting trend, data patterns or variance analysis
Data warehouse
Data warehouse is an information storage collected from multiple sources & is stored in a unique schema
Data warehouse is constructed by a process including cleansing, integration, transformation, load & regular data refresh.
Data in data warehouse is organized in & object-oriented approach. Data is summarized & is stored to provide information in historical view for decision support (for organization)
Data warehouse is modeled by a multidimensional data, called data cube
Each dimension is 1 or a set of attributes in schema
Each cell stores a value like count or sum
A data cube provides a multidimensional view & allow pre-computation & quick access of summarized data
Data warehouse (cont)
Data warehouse supports OLAP
- OLAP is based on domain knowledge to represent data in various abstract levels. 2 fundamental operations in
OLAP are drill-down & roll-up allowing users to observe data in different summarization level. E.g:
Drill-down observes monthly data from quarter data
− Roll-up observes country data from province data
General multi-dimensional observation techniqes can combine multiple dimensions at different detail levels.
That leads to discovery important patterns
Transaction data
A record in a transaction DB represents a transaction, including a unique identifier & components involving in transaction
- Typical transaction types include transfer, purchase, buying, order, mouse click
- Transactional DB might miss additional tables like seller information or branch information
- Transactional data mining focuses on detection frequent sets. E.g, answering question “Which products are bought (buy customers) together?"
Other data types
Temporal data (stock), sequence (biology), space (map), industry design (construction design, system components, board), hypertext & multimedia, graph & networks
- Challenges on data structure (sequence, tree, graph, network) & semantic (order, connectivity)
- Some applications:
Temporal data: Detect transaction trend to plan customer care, money distribution; Detect stock trend for investment; Abnomaly detection based on clustering or occurrence frequency
Spatial data: Estimate poverty rate based on distance to main road;
Detect community based on distance between items
Textual data: Estimate customer satisfaction based on review content
Multimedia: Object detection & classification, goal detection in videos
2.4 Discoverable patterns
Functions in data mining: Describe & discriminate data, frequent pattern mining, association & correlation, classification & regression, clustering & abnomaly detection
- 2 types of data mining tasks: descriptive & predictive
Describe & discriminate data
Data description is summarizing common characteristics or features of a class of data
Data statistics
Roll-up in OLAP describe data in a specific dimension. E.g: Summarizing common characteristics of customer paying more than 100 million/year → 40-50 years old, job (could drill down in job dimension)
Attribute oriented inference allows generalizing & describing data without user interaction step by step
Data discrimination is comparing general features of a class with other reflective classes.
Reflective classes are provided by user. Their data could be retrieved from DB.
Comparative description could be represented by rules
E.g: Compare customer who frequently vs rarely buy hi-tech → 80% customers frequently buy hi-tech are at 20-40 & have college degree, 60% customers rarely buy hi-tech don't have college degree, drill-down in education or income could have other useful information
Frequent patterns, association & correlation
Set of frequent items includes items frequently occurring together in a transactional DB (e.g, milk & bread are frequently bought together at food stores); frequent sequential patterns (e.g: customers usually buy computer, camera & memory card in that order); frequent structural patterns
Association analysis mua(X, máy tính) → mua(X,phần mềm) [support = 1%, confidence = 50%)
Những sản phẩm nào thường được mua cùng nhau trong cùng một giao dịch
X: khách hàng
Độ tự tin (độ chắc chắn) (confidence/certainty) thể hiện khả năng khách hàng mua phần mềm nếu biết khách hàng mua máy tính
Độ hỗ trợ (support) thể hiện tỉ lệ giao dịch mà máy tính và phần mềm được mua cùng nhau trên tổng số giao dịch được phân tích
Luật kết hợp theo một chiều mua máy tính → phần mềm [1%, 50%]
Luật kết hợp đa chiều (liên quan đến nhiều thuộc tính) tuổi(X,20..29)^thu nhập(X,20..30tr)→mua(X,laptop) [2%,60%]
Có 2% có tuổi 20-29, thu nhập 20-30tr đã mua laptop trong tổng số khách hàng được phân tích (thu thập từ
CSDL quan hệ)
Có 60% khả năng những người trong độ tuổi 20-29 và có thu nhập 20-30tr sẽ mua laptop
Luật kết hợp không thỏa mãn nếu ở dưới ngưỡng độ hỗ trợ tối thiểu (minimum support threshold) và đồ tự tin tối thiểu (minimum confidence threshold)
Classification & regression
Classification is process of searching a model to describe & discriminate data classes
Model is induced from training data (items with class labels)
− Model is used to predict class labels of unknown items
− Model could be represented by rules, decision trees, math/probabilistic equations, or neural networks
Regression modelizes continuous functions
Clustering
Items are clustered by maximizing intra-cluster similarity & minimizing inter-cluster similarity
- A cluster could be considered as a class to induce regularity
- Clustering is used in taxonomy construction
Abnomaly detection
Abnomaly are items that don't fit to common behaviour or model of data set
- Most
DM techniques consider abnomaly as noise or exception. However, in some applications like fraud detection, these items are importants
- Typical statistics tests could detect abnomaly based on data modeling following a distribution or a probabilistic model
- Distance-based methods detect abnormal items far from data clusters
- Density-based methods could detect abnormal items in a region where general statistic distribution couldn't detect
- E.g: Fraud card transaction could be detected based on transaction volume when comparing to normal transactions; or based on information of location, transaction type or transaction frequency
Potential patterns
Potential patterns
A potential pattern represents knowledge
Objective metrics based on structure & statistics of patterns
In association analysis, support of an association rule X => Y represents transaction fraction satisfying rule. Confidence represents probability P(Y|X)
In classification, accuracy of a classification rule represents data fraction correctly classified; coverage represents data fraction satisfied rule
Subjective metrics based on user belief on data i) understandable ii) correct in new data or pivot data with a certainty iii) potentially useful iv) novel or confirm an assumption from user
Not expected by users (vs belief)
Provide strategic information for decision support (e.g “a earthquake is typical followed by aftershocks")
As expectation to confirm an assumption from users
A DM system could generate all potential patterns?
A DM system could generate only potential patterns?
2.5 Techniques in DM
Statistics
Machine
Learning
Pattern
Recognition
Visualization
DBMS
Data mining
Data warehouse
Algorithm
Information
Retrieval
High
Performance
Computing
Applications
Statistics
Study collection, analysis, explanation & visualization of data
Statistic model is a set of function describing behaviour of items in a target class in terms of random variables & relevant probability distribution. Fnferential statistics modelize data using stochastic process & uncertainty of observations
 output of data description & discrimination coud be statistic models
Pattern mining coud use statistic models to detect & process noise & missing data
 output of DM could be verified by statistical hypothesis test.
 output is statistically significant it has a small chance to be generate by random
Most statistic methods have high computational complexity
Machine learning
Study learning ability of machine (or improve learning results) using data
A main research direction is to make computer programs to automatically recognize complex patterns in data & make intelligent decisions based on data
Supervised learning (classification) is based on labeled data
Unsupervised learning (clustering) uses unlabeled data
Semi-supervised learning is based on both labeled & unlabeled data. An approach is to use labeled data to build model & use unlabeled data to tune class boundary
Active learning: Allow user participating in learning process. It aims at optimizing model quality under a constraint on number of labeled data
Machine learning (cont) outlier
Boundary based on labeled data
Boundary based on unlabeled data
DBMS & data warehouse
DBMSs create, operate & use Dbs for organizations & end-users. DBMSs establish principles on data modeling, query language, query processing, optimization, data storage, indexing & acess. DBMSs could process data with complex structure
- DM require processing big data, in real-time & streaming data. DM could applied techniques in DBMS & integrate DM functions into DBMSs
- Data warehouse integrates data from multiple sources, supports OLAP operations & multi-dimensional DM
Information retrieval
Science of searching for documents or information inside documents
- Documents are texts or multimedia (image, sound, video)
- 2 assumptions i) data is non-structural ii) query includes kewords & doesn't have complex structure
Topic models detect main topics in a document collection as long as in each document
2.6 Applications of DM
Business intelligence
Organizations require knowledge about their business environment: customer, market, supply, resource & competitors
Business intelligence provides historical & present view & predicts organization operations
Business intelligence includes report, OLAP, business performance management, competitive intelligence, standardization & predictive analysis
DM helps organization effectively analyze market, compare customer feedbacks to similar products, analyze strength & weakness of competiors, keep close customers & make wise business decision
OLAP tools based on data warehouse & multi-dimensional DM; classification & predictive analysis play a center role in analyzing market, supply & sale; clustering is applied in customer relationship management to group similar customers; data description techniques could be use to understand customer groups to provide suitable services
Web search
DM techniques are applied in search engine including crawling (which pages & frequency), indexing (which page & which part), search (ranking, advertisement, personalization)
- Search engines use cloud computing infrastructure with thousands nodes
- Search models & query classifiers are built offline; queries are processed online; models & classifiers are update based on change of queries & web data
- Personalizing search results give answers based on user profile & search history; rare queries are challenging
2.7 Challenges in DM
Mining methods
Mining new & various knowlege: Combine clustering & ranking to create high-quality clusters & ranking in large networks
Mining in multi-dimensional space by combining attribute dimensions
Combining techniques from relevant fields: natural language processing in text mining, software engineering in bug mining
Mining in semantically linked environment: knowledge from a set of items enforce mining knowledge from related items
Noise, uncertainty, missing data: Could negatively effect DM & generate wrong patterns. Data cleansing, preprocessing, abnomaly detection, uncertainty inference could be applied.
Pattern evaluation based on subjective criteria; guide mining process with potential patterns or with user to increase pattern quality & limit search space
User interaction
Flexible user interface, help users easily interact with system.
Data sampling, analyzing common characteristics of data, estimating mining results
Changing search objectives, tuning mining requirement
Fundamental knowledge, constraints, regularity & other information of application domain need to be integrated into system to evaluate patterns or to guide mining process
- Develop DM languages to do ad hoc mining taks & to support data description
- Represent & visualize results lively & flexibly so that users understand results & use them directly in decision making
Efficace & extendability
DM algorithms need to be effective & extendable to extraction information from a large volume of data in a dynamic environment. Execution time needs to be predictable , short & acceptable
- Parallel & distributed computing: Data is separated into pieces & is processed in parallel processes; processes could communicate to share data & information; Their results are then combined
- Incremental mining allow updating new data without restart mining process from beginning to enrich mined knowledge
Effects of DM on society
Limit bad effects, bring benefit to society
- Evaluate data sensitivity, ensure data policy
- Integrate DM into current systems to increase service quality without requiring user knowledge on DM techniques
3. What is web data mining?
Web is largest public data source
- Main characteristics of web data:
1. volume of information/data in Web is increasingly huge.
Information coverage is large & various. We could find almost everything in web
2. Many data types: Structured tables, semi-structured web pages, nonstructural texts, multimedia files (image, sound, video)
3. Information in Web is heterogeneous. Different web pages could show same information or similar information in different format. This makes information integration complicated.
4. Information in Web is linked. Hyperlinks between web pages exist inside a website or between websites. In a website, hyperlinks form an
(internal) information organization mechanism. Between websites, hyperlinks implicitly role of web pages (densely linked web pages typically have high quality &/or high impact)
What is web data mining (cont)
5. Information on Web is misleading:
A typical web page contains pieces of information (main content, redirection, ad, copyright, user policy, v.v.)
Web basically doesn't have content proof, anyone could publish anything in
Web. Most content in Web has low quality, errors, or incorectness
6. Web is a business & commercial channel: All commercial website provide users with buying, purchase, information collection. Websites need to automate tasks (e.g suggestion, customer care) to improve effectiveness.
7. Web is dynamic. Content in Web continuously changes. Follow & monitor change is an important requirement to web apps
8. Web is a virtual society. It is not only data, information, services, but also interaction between human, organization & automatic systems.
User could easily & immediately connect to anyone from anywhere in world. They could express their opinion on anything in forums, blogs, review pages, or social networks. That information create a new type of data for DM or social network analysis
What is web data mining (cont)
Study information distribution in web
- Study characteristics & classify web pages
- Monitor web evolution
- Study relationship between web agents like web pages, users, communities & activities in web
Lesson 2:
Machine Learning
Content
1. Basic Concepts
2. Evaluation method
3. Decision tree
4. Naive Bayes Algorithm
5. SVM Algorithm
6. KNN Algorithm
7. Feedforward neural network
8. Convolutional neural network
9. Recurrent neural network
10. Ensemble classifiers
1. Basic concepts
 data is described by attributes in set A = {A1, A2, ..., A|A|}
Class attribute C = {c1, c2, ..., c|C|} (|C| ≥ 2), ci is a class label.
Each learning dataset includes examples containing information about “experience".
Given a dataset D, goal of learning is to build a classifier function /predictor associates attribute values in A with classes in
C.
Functions can be used to classify/predict unseen data
 function is also known as a classifier/prediction model or classifier
Table 1
ID
Age
Young
Middle-age
Old
Have job
FALSE
TRUE
FALSE
TRUE
FALSE
TRUE
FALSE
Have home
FALSE
TRUE
FALSE
TRUE
FALSE
Credit
Normal
Good
Normal
Good
Excellent
Good
Excellent
Normal
Class
No
Yes
No
Yes
No
Supervised Learning: Class labels are provided in dataset
 data used for learning is called training data
After model is learned through a learning algorithm, it is evaluated on a test dataset to measure its accuracy.
Do not use test data to learn model.
 labeled data set is usually divided into 2 independent sets for training & testing.
Number of correctly classified samples
Accuracy =
Size of test dataset
What is machine learning?
Given a dataset representing past "experience", a task T & a performance metric M. A computer system is capable of learning from data to perform task T if after learning performance of machine on task T (measured by M) is improved.
 learned model or knowledge helps system perform task better than no learning at all.
 learning process is process of building models or knowledge distillation.
In Table1, if there is no learning process, assume that test dataset has same class distribution as training data.
Make random predictions → Accuracy = 50%
- Make predictions according to most popular class
(class Yes) → Accuracy = 9/15 = 60%
 model is capable of learning if accuracy is improved
Relationship between training & test dataset
Assumption: distribution of training dataset & test dataset is same.
Training
Dataset
Learning
Algorithms
Step 1: Training
Model
Test dataset
Step 2: Test
Accurac
2. Evaluation methods
2.1 Evaluation methods
Split data into 2 independent training & test sets
(usually using a 50-50 or 70-30 ratio)
Random sampling to generate training set; rest as test set
If data is built over time, use past data as training data
If dataset is small, perform sampling & evaluation n times then average
Cross-validation: data is divided into n equal independent parts. Each time, 1 part is used as test data & n-1 remainder as training data. results are averaged.
Leave-1-out: If data is too small, each set contains only
1 element, number of parts = number of elements in data set.
Validation set: Used to select model's hyperparameters
(parameters not learned)
2.2 Evaluation Metrics
Confusion matrix
Predicted Positive
Predicted Negative
Actually Positive
TP
FN
Actually Negative
FP
TN
TP: actual positive, predicted positive (true positive)
TN: actual negative, predicted negative (true negative)
FP: actual negative, predicted positive (false positive)
FN: actual positive, predicted negative (false negative)
A positive example is an example with a class label of interest
A negative example is an example with a class label of disinterest
Precision =
TP
TP + FP
Recall =
TP
TP +
FN
F=
2pr p+r
Assume: 10 positive texts
Rank 1: p = 1/1 = 100%
Rank 2: p = 2/2 = 100%
Rank 9: p = 6/9 = 66.7%
Rank 10: p = 7/10 = 70%
7/10 = 70% r= r= r = point
Break-even r=
TP
TP +
FN
(sensitivity
TPR =
FPR =
FP
TN + FP
= (1- specificity)
TNR =
TN
TN + FP
(specificity
ROC curve of 2 classifiers c1 & c2 on same set of dataset
 corresponding lift curve of data in table
3. Decision Tree
Decision node: leaf node.
For prediction, traverse tree from root by values of attributes until a leaf node is encountered.
Age
Young
Have job? tru
Yes
Age
Young
Class
FALSE
Ol
Middleage
Have home? fals tru
No
Yes
Have job
FALSE
Credit? fals
Norma
No
Have home
No
GOOD
Go od
Yes
Credit
Excellent
Yes
Decision trees are built by dividing data into homogeneous subsets. subset is said to be homogeneous if examples have same class.
Small trees are generally more general & more precise; easier to understand for humans.
 tree is not only 1 born.
Finding best tree is an NP-complete problem → using heuristic algorithms.
Have home= true → Class =Yes
[sup=6/15, conf=6/6]
Have home = false, Have job = true → Class = Yes
Have home = false, Have job = false → Class = No
Age = Young, Have job = false → Class = No
Have home? tru
Yes
(6/6 tru
Yes
(3/3 fals
Have job? fals
No
[sup=3/15, conf=3/3]
[sup=6/15, conf=6/6].
[sup=3/15, conf=3/3]
3.1 Learning algorithm
Use divide-&-conquer to divide data recursively
Stop condition: all examples have same class or all attributes are already used (line 1-4)
At each recursive step, choose best attribute to split data by attribute's value based on heterogeneity function (line 7-11)
Greedy Algorithm
Algorithm decisionTree(D, A , T) if D contains only training example of class cj ∈ C then create leaf node T with class label cj; elseif A = ∅ then create leaf node T with class label cj being most common class in D else
// D contains examples with multiple classes. Select an attribute
// to split D into subsets so that each subset is more homogeneous. p0 = impurityEval-1(D); for each attribute Ai ∈ A (={A1, A2, ..., Ak}) do pi = impurityEval-2(Ai, D)
10 endfor
Chose Ag ∈ {A1 ,A2 , ..., Ak} minimize heterogeneity by p0 - pi;
12 if p0 - pg < threshold then
// Ag does not significantly reduce heterogeneity p0
13 create leaf node T with class label cj being most common class in D
14 else
// Ag reduces heterogeneity p0
Create a decision node T according to Ag;
Let values of Ag be v1, v2, ..., vm.Split D into m non-intersecting subsets D1, D2, ..., Dm based on m values of Ag.
17 for each Dj in {D1, D2, ..., Dm} do
18 if Dj ≠ ∅ then
19 create a branch (edge) corresponding to node Tj for vj that is a child of T;
20 decisionTree{Dj, A - {Ag}, Tj)
// delete Ag
21 endif
22 endfor
3.2 Nonhomogenous function
Age?
Have home?
Youn
Middle
Age
Ag tru fals
No:
Yes: a)
Yes:
Yes: b) of 4tree a) is higher
6 than that of tree
Nonhomogeneity b)
|C| entropy(D) = j=1
ΣPr(c )log Pr(c
|C| j=1
Pr(cj) = Σ Pr(cj)
- Pr(cj) is probability that data belongs to class cj
- unit of entropy is bit
- Convention 0log20 = 0
- more homogeneous data, smaller entropy & vice versa.
Example 6:
Dataset D has 2 class positive (pos) và tiêu cực (neg)
1. Pr(pos) = Pr(neg) = 0.5 → entropy(D) = -0.5 x log20.5 – 0.5 x log20.5 = 1
2. Pr(pos) = 0.2, Pr(neg) = 0.8 → entropy(D) = -0.2 x log20.2 – 0.8 x log20.8 =
3. Pr(pos) = 1, Pr(neg) = 0 → entropy(D) = -1 x log21 - 0 x log20 = 0
Information gain
1. Calculate entropy(D) (line 7)
2. Attribute selection: For each attribute Ai, assuming there are v values, split D into nonintersecting subsets D1, D2, …, Dv (line 9) v |D entropyAi(D) = Σ | x entropy(Dj) j=1
|D|
3. Information gain Ai gain(D, Ai) = entropy(D) entropyAi(D)
entropy(D) = -6/15 x log26/15 – 9/15log29/15 = 0.971 entropyAge(D) = 5/15 x entropy(DAge=Young) + 5/15 x entropy(DAge=MiddleAge)
+ 5/15 x entropy(DAge=Old)
= 5/15 x 0.971 + 5/15 x 0.971 + 5/15 x 0.722 =
Entropyhave home(D) = 6/15 x entropy(Dhave home=true)
+ 9/15 x entropy(Dhave home=false)
= 6/15 x 0 + 9/15 x 0.918 = 0.551
Entropyhave job(D) = 0.647 entropycredit(D) = 0.608 gain(D, Age) = 0.971 - 0.888 = 0.083 gain(D, Have home) = 0.971 - 0.551 = 0.420 gain(D, Have job) = 0.971 - 0.647 = 0.324 gain(D, Credit) = 0.971 - 0.608 = 0.363.
Information gain ratio
Information gain often favors attributes with multiple values
Gain ratio normalized entropy over attribute values s: number of different values of Ai
Dj is a subset of D whose attribute Ai has value j
Gain(D, Ai)
GainRatio(D, Ai) = s D
-Σ j=1
+ j
3.3 Handle persistent attributes
Split attribute into 2 intervals (binary split)
 dividing threshold is chosen to maximize information gain (ratio)
During tree creation, attribute is not deleted (line 20)
Y2
3.4 Some advanced issues
Overfitting: A classifier f is said to be overfit if there exists a classifier f' whose accuracy f > f' trên DL on training set but < on test set
Cause: data contains noise (wrong class label or wrong attribute value) or complex classification problem or contains randomness.
Pruning: decision tree is too deep → prune tree by estimating error at each branch, if error of subtree is larger then pruning. An independent data set (validation set) can be used for pruning. Alternatively, pre-pruning or law pruning can be applied
Missing value: Use “unknown" or most common value (or average with continuous attribute)
Class imbalance: Over sampling, ranking
Rule 1: X ≤ 2, Y > 2.5, Y > 2.6 →
Rule 2: X ≤ 2, Y > 2.5, Y ≤ 2.6 →
Rule 3: X ≤ 2, Y ≤ 2.5 →
X ≤ 2, Y > 2.6 →
X≤2→
Y2
4. Naive Bayes Algorithm
4.1 Naive Bayes Algorithm
Given a attribute set A1, A2, ..., A|A|, C is a class attribute with values c1, c2, …, c|C|, example testd = <A1=a1, …, A|A|=a|A|>
Assumption MAP (maximum a posteriori): find class cj such that
Pr(C=cj|A1=a1, ..., A|A|=a|A|) Given an attribute set A1, A2, ..., A|A|, C is a class attribute with values c1, c2, …, c|C|, for example test d =
<A1=a1, …, A|A|=a|A|>Assumption MAP (maximum a posteriori): find class cj such that Pr(C=cj|A1=a1, ..., A|A|=a|A|) is maximal
Pr(C=cj| A1=a1,..., A|A|=a|A|) =
Pr(A1=a1, ..., A|A|=a|A|| C=cj) x Pr(C=cj)
Pr(A1=a1, ..., A|A|=a|A|)
Pr(C=cj| A1=a1,..., A|A|=a|A|) ∝ Pr(A1=a1, ..., A|A|=a|A|| C=cj) x Pr(C=cj)
Pr(A1=a1, ..., A|A|=a|A|| C=cj) = Pr(A1=a1| A2=a2,..., A|A|=a|A|, C=cj) x Pr(A2=a2,...,
A|A|=a|A|| C=cj)
= Pr(A1=a1| A2=a2,...,
A|A|=a|A|, C=cj) x Pr(A2=a2|
A3=a3,..., A|A|=a|A|,C=cj)
Conditional Independence Assumption:
A|A|=a=a
| C=cj) ,..., A =a , C=c ) = Pr(A =a | C=c )
Pr(A
1 |A|1| A2=a
|A|
Pr(A2=a2| A3=a3,..., A|A|=a|A|, C=cj) = Pr(A2=a2| C=cj)
... x Pr(A3=a3,...,
Pr(A1=a1, ..., A|A|=a|A|| C=cj) = Pr(A1=a1| C=cj) x Pr(A2=a2| C=cj) x … x Pr(An=an| C=cj)
Pr(C=cj| A1=a1,..., A|A|=a|A|) ∝ Pr(A1=a1| C=cj) x Pr(A2=a2| C=cj) x … x Pr(An=an| C=cj) x Pr(C=cj)
Pr(C=cj) = number of examples with class cj total number of examples in dataset number of examples with Ai =ai & class cj
Pr(Ai=ai| C=cj) = number of examples cj
Pr(C = t) = 1/2
Pr(A = m| C = t) = 2/5
Pr(A = g| C = t) = 2/5
Pr(A = h| C = t) = 1/5
Pr(A = m| C = f) = 1/5
Pr(A = g| C = f) = 2/5
Pr(A = h| C = f) = 2/5
Pr(B b|=Cq,= C t) =
Pr(B = s| C = t) = 2/5
=m,= B
Pr(B = q | C = t) = 2/5
Pr(B =
= t| b|AC==m, f) =B 2/5
Pr(B
=C s| =Ct)= xf)Pr(B
= 1/5= q| C =
Pr(C
= q) ∝ Pr(C = t) x Pr(A
= m|
Pr(B = q | C = f) = 2/5 t)
Pr(C = f) = 1/2
∝ 1/2 x 2/5 x 2/5 =
Pr(C = f| A = m, B = q) ∝ Pr(C = f) x Pr(A = m| C = f) x Pr(B = q| C =
Probability – 0: attribute value ai is not x/h of same class cj in training data makes Pr(Ai = ai|C = cj) = 0 nij + λ
Pr(Ai = ai| C = cj) = nj + λ x mi where nij is example number with Ai = ai và C = cj; mi is number of different values of attribute
Ai
- λ = 1/n where n is number of training examples
- λ = 1: Laplace smoothing
4.2 Text classification based on NB
Probabilistic generation model: Assume that each document is generated by a distribution according to hidden parameters.
These parameters are estimated based on training dataset. parameters are used to classify test text by using Bayes' law to calculate posterior probability of class that is likely to produce text.
2 assumptions: i) dataset is generated by a mixing model ii)
Each mixing component corresponds to a class. lớp lớp
 probability distribution function of 2 Gaussian with parameters θ1 = (μ1, σ1) và θ1 = (μ1, σ1)
Suppose there are K mixed components, component j has parameter θj, parameter of whole model includes Θ = (φ1, φ2, …, φk, θ1, θ2, …, θk) where φj is weight of component j (Σφj = 1)
- Assuming there are classes c , c , …, c
|C|, we have
|C| = K, φj = Pr(cj|Θ), text generation process di:
1. Select a mixing component j based on a priori probabilities of classes, φj = Pr(cj|Θ)
2. Generate di based on distribution Pr(di| cj; θj)
- Probability of generating d i based on whole model:
|C|
Pr(dj| Θ) =
Pr(cj| Θ) x Pr(di| cj; j=1 θj)
Text is represented as a bag of words
1. Words are generated independently, independent of context (other words in text)
2. Probability of word does not depend on position in text
3. Length & class of text are independent of each other
- Each text is generated by a polynomial distribution of words with n independent trials where n is length of text.
Polynomial test is a process that generates k values (k ≥ 2) with probabilities p1, p2, …, pk
Example: dice produce 6 values 1, 2, …, 6 with fair probability p1 = p2 = … = p6 = 1/6)
- Suppose there are n independent trials, let Xt be number of times value tis generated, then X1, X2, …, Xk are discrete random variables.
(X1, X2, …, Xk) follows a polynomial distribution with parameters n, p1, p2, …, pk
- n: document length |di| k = |V|: dictionary length pt: Probability of word wt x/h in document, Pr(wt| cj; Θ)
Xt: Random variable representing number of times wt x/h in document
Nti: Number of times wt x/h in di
Probability Function:
Parameter estimation:
Lidstone smoothing (λ < 1)
(λ = 1: Laplace smoothing)
Classify:
5. SVM Algorithm
Support vector machine (SVM) is a linear learning system for building 2-class classifiers.
Example set D: {(x1, y1), (x2, y2), ...(xn, yn)} where xi = (xi1, xi2, …, xir) input vector r-dimension in space
X ⊆ Rr, yi is class label, yi ∈ {1, -1}
SVM constructs a linear function f: X Rr →R với w = (w1, w2, …, wr) ∈ Rr f(x) = <w ⋅ x> + b yi if f(xi) ≥ 0
-1 if f(xi) < 0
5.1 Linear SVM: divisible data w: normal vector of hyperplane
SVM finds hyperplane maximize margin
H+: <w ⋅ x> + b = 1
H-: <w ⋅ x> + b = -1 to
Structural risk minimization principle: Marginal maximization minimizes upper bound of error (classification) x+
||w||
|b
||w||
<w ⋅ x> + b = 0 y = -1 d+ = d- = y=1
Hyper-plane dividing 2 layers
There are countless such hyperplanes, how to choose?
What to do if data is not linearly divisible? y=1
<w ⋅ x> + b = 0 y = -1
Constrained minimization problem
Minimization :
<w ⋅ w>/2
Constraint : yi(<w ⋅ xi> + b) ≥ 1 i = 1, 2, …, n
Since squared & convex objective functions, constraints are linear, we use Lagrange multiplier method where αi is a Lagrange multiplier
General problem
Minimization:
Lagrangian
Constraints:
1, 2, …, n f(x) gi(x) ≤ bi i=
Conditions Kuhn -Tucker:
∂L
∂x
2, …, r j = 1, gi(x) - bi ≤ 0,
…, n i = 1, 2, αi ≥ 0, i = 1, 2, …, n αi(bi - gi(x) = 0, i = 1, 2, …, αi > 0 corresponds to dataset called support vectors
Due to convex objective function & linear constraints
Kuhn - Tucker conditions are necessary & sufficient, replacing original problem with a dual problem (Wolfe duality)
Maximize
Constraints
Hyperplane
Classify examples z:
(sv: set of support vectors)
5.2 Linear SVM: Indivisible dataset
Minimization:
Constraints:
||w|| yi(<w ⋅ xi> + b) ≥ 1 – ξi, i = 1, 2, …, n ξi ≥ 0, i = 1, 2, …, n
Lagrangian
|ba
||w||
<w ⋅ x> + b = 0
Conditions Kuhn – Tucker:
Dual Problems
Maximize:
Constraints:
Hyperplane
Comments: αi = 0 → yi(<w⋅x> + b) ≥ 1 và ξi = 0
0 < αi < C → yi(<w⋅x> + b) = 1 và ξi = 0 αi = C → yi(<w⋅x> + b) ≤ 1 và ξi ≥ 0
5.3 Nonlinear SVM: Kernel Function
{(Φ(x1), y1), (Φ(x2), y2), …, (Φ(xn), yn)}
Φ: X → F x ↦ Φ(x)
Minimization
Constraints yi(<w ⋅ Φ(xi)> + b) ≥ 1 – ξi, i = 1, 2, …, n ξi ≥ 0, i = 1, 2, …, n o o
Φ(o)
Input space
Φ(x
Φ(o)
Φ(x
Φ(o)
Feature space F
Φ(x
Dual Problems
Maximize
Constraints
Classify
Eg: (x1, x2)
(x12, x22, 21/2x1x2)
Kernel function: a vector multiplication on input space corresponds to a vector multiplication on a multidimensional feature space
K(x, z) = <Φ(x) ⋅ Φ(z)>
Polynomial kernel:
K(x, z) = <x ⋅ z>d
Eg: x = (x1, x2), z = (z1, z2)
<x ⋅ z>2 = (x1z1 + x2z2)2
= x12z12 + 2x1z1x2z2 + x22z22
= <(x12, x22, 21/2x1x2) ⋅ (z12, z22,
21/2z1z2)>
= <Φ(x) ⋅ Φ(z)>
Feature space of kernel polynomial of degree d has Cdr+d-1p dimension
Mercer theorem defines kernel functions
Polynomial kernel:
K(x, z) = (<x ⋅ z> + θ)d
Gaussian RBF:
In order for SVM to work with discrete properties, it is possible to convert to binary
For multiclass classification, strategies such as 1-vs-all or 1-vs-1 can be used
Hyperplane is confusing for users, so SVM is often used in applications that don't require explanation
6. kNN
Given training dataset D, a test example d
1. Calculate similarity (distance) of d with all training examples.
2. Determine k closest examples based on similarity
3. Classify d based on labels of k examples above
Defect:
Long sorting time
− Difficult to explain
3-nearest neighbor
2-nearest neighbor
1-nearest neighbor
Lesson 2:
Machine Learning
(cont.)
7. Feedforward Neural Network
Artificial Neural Network (ANN) simulates biological neural system, which is a network of interconnected artificial neurons. ANN can be considered as a distributed & parallel computing architecture
Each neuron receives input signals, performs local computations to form output signal. output value depends on characteristics of each neuron & its connections with other neurons in network.
ANNs perform learning, memory & generalization by updating weights of connections between neurons.
 objective function depends on network architecture, characteristics of each neuron, learning strategy & learning data
Perceptron x0 = w0 = x1 =
Bias x2 =
... su xn =
Weights
Activation function
Input signals
Sigmoid activation function f(u) =
1 + e-α(u + θ)
Popular
■ Slope parameter α
■ Value between (0,1)
■ Continuous & continuous derivative
ANN structure
ANN network architecture is defined by:
Eachlayer consists of a group of neurons
Number input/output signal
Number of layers
Number of neurons in each layer
Connection between neurons
Input layer receives input signals
Output layer return output signals
Hidden layers between input & output layer
In Feedforward Neural Network connection comes from previous layer to following layer. There is no backward or intra-layer connection.
FNN example
Input layer
Bias
Hidden layer
Output layer
A 3-layer FNN
- Input layer consists of 4 neurons
- Hidden layer consists of 5 neurons
- Output layer returns 2 output signal as it consists of 2 neurons
Number of parameters: 4 x 4 + 5 x 2 = 26
(practical neural network has ~106 parameters)
Loss function
Consider an ANN with 1 output signal
■ Input (x, y) as input signal & ground-true output
■ Loss function
Ex(w) = 1/2(y-y')2 where y' is model's output
■ Loss function of whole dataset D
ED(w) = 1/|D| ∑x∈DEx(w)
Gradient descent
Gradient of loss function E (denoted as ∇E) is a vector proportional to slope of E
∇E determines fastest direction to increase value of E
∇E(w) =
∂E
∂w1 ∂w2
∂wn where n is number of parameters
Find fastest direction to decrease value of E
Δw = -η.∇E(w) where η is learning rate
Activation function must be continuous & continuous derivative
Backpropagation
Perceptron only perform linearly separable functions
■ Multi-layer neural network can perform non-linearly separable functions
■ Backpropagation
Propagate input signal from input layer through layers to output layers
Backpropagate error signal:
Evaluate error of output layer
Error signal backpropagate through layers back to input layer
Error values are evaluated depending on local error of each neuron
Gradient descent algorithm
Algorithm Gradient_descent_incremental((D, η))
Init w (wi ← small random value) do for each example (x, d) ∈ D do
Compute output for each parameter wi do wi ← wi – η(∂Ex/∂wi ) endfor endfor until (terminal condition)
10 return w
Initialize weights
Random
■ Large weight causes sigmoid function saturation.
Objective function
■ w ab (connect from neuron b to neuron a) w0ab ∈ [−1/na, 1/na] where na is number of neurons in layer containing a w0ab ∈ [−3/ka1/2, 3/ka1/2] where ka is number of neurons in layer containing b
Learning rate
Large learning rate accelerates learning process, but can overlook global optimum
■ Small learning slow down learning process
■ Learning rate is often chosen based on experiment
Change learning rate during learning process
Number of neurons in hidden layers
Number of neurons is chosen based on experiment
■ Start with small number
■ Increase if model can not converge
Consider decreasing if model converged
Learning limitation of ANN
1-hidden-layer ANN can perform binary functions
■ 1-hidden-layer ANN can perform stricted continuous functions
■ 2-hidden-layer ANN can perform continuous functions
Pros & cons
Pros:
Support parallel computing
Fault tolerant
Self-adaptive
Cons:
Network structure & hyper parameter chosen by experiment
A blackbox method, no clue to explain exactly what happened inside
Hard to explain to customer
Applications of ANN
High dimensions input
■ Output has type of number, vector
■ There is noise in data
Results explanation is not mandatory
■ Long training time is acceptable
■ Requires promptly predictions
8. Convolutional Neural Network
Handwritten number recognition [0..9]
MNIST dataset:
Input: picture
Output: number [0..9]
Picture size 28 x 28
Train dataset size: 60K
Test dataset size: 10K
FNN can not utilize spatial relation of pixels in picture
Filter
Convolutional
Neural
Network (CNN) simulates visual activity
Input as 28 x 28 matrix
Each neuron in hidden layer receives input signal from a 5x5 area (25 pixel)
Stride filter on input picture, each area connect to a neural on hidden layer
Hidden layer has 24x24 neurons tầng đầu vào oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo nơ-ron tầng oẩn
Example with 5x5 filter
Input layer oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo oooooooooooooooooooooooo
Hidden layer oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo oooooooooooooooooooooo
Parameters sharing
Weights & biases are share among filters
■ Hidden layer is supposed to detect a visual feature at different areas of input picture, based on translational invariance of pictures
■ Filter is sometimes called kernel
■ Output of hidden layer is called feature map
■ Different filter results in different feature maps
Example of 3 filter
Input layer 28 x 28
Convolution layer
3 x 24 x 24
Example of filter
 whiter little square, smaller value
■ darker little square, higher value
■ Filter learn patterns in picture
Number of parameters
Fewer number of parameters than FNN
■ A filter has 5x5 =25 weights & 1 bias
■ 20 filters 20 x 26 = 520 parameters
A FNN has hidden layer of 30 neurons has 30 x 28 x 28
(connection) + 30 (bias) = 23,550
Pooling layer
Pooling layer is installed after convolution layer
■ Aggregate important features of convolution layer
■ E.g.: feature map divided into 2x2 squares. Pooling layer return largest value in each square & aggregate into a smaller feature map (max pooling)
■ Pooling is applied to every filter
■ Pooling layer is supposed to remove positional information & keep useful pattern
■ L2 pooling: square root of sum of square of values
Example of pooling layer
Input layer 28 x 28
Convolution layer
3 x 24 x 24
Pooling layer
3 x 12 x 12
Example of full network
Input layer 28 x 28
Convolution layer
3 x 24 x 24
Output layer
(softmax)
Pooling layer
3 x 12 x 12
Fully connected
Softmax activation function
Tackle problem of inefficient of mean square error
■ z j is input signal of j-th neuron in output layer L
L is output signal of j-th neuron of
■ α output layer L after softmax
■ log-likelihood loss function
Generalizability of CNN
Learn high-level abstraction by convolution & pooling
■ Activation function chosen based on experiment
■ Activation function & loss function: sigmoid - cross entropy vs softmax - log likelihood
Techniques to avoid overfitting
9. Recurrent Neural Network
Sometimes, output depends on current input & other previous outputs
■ Time-series problem (stock prices, weather forecast,…)
■ Speech processing & natural languague processing
(speech recognition, part-of-speech tagging, sentiment analysis)
Recurrent Neural Network
Recurrent Neural Network is able to store information in recent history
■ State of neuron at time (t-1) is used as input value at time t
■ Parameters are shared by time
Example of RNN
... xI xI xI xI
Number of parameters
I: number of neurons of input layer
■ H: number of cell RNN
■ K: number of neuron of output layer
Number of parameters: I x H + H x H + H + H x K + K
Connection between input & hidden layer: I x H
Number of recurrent connection in hidden layer: H x H
Number of connection between hidden & output layer: H x K
Number of hidden layer biases: H
Number of output layer biases: K
Backpropagation: Forward phase where:
- xti is input signal of i-th neuron at time t
- wih is connection weight between i-th neuron of input layer & h-th neuron of hidden layer
- wh'h is connection weight between h'-th neuron & h-th neuron of hidden layer
- bh't-1 is output signal of h'-th neuron at time (t-1)
- αth is input signal of h-th neuron of hidden layer at time t
where:
- bth is output signal of h-th neuron of hidden layer at time t
- θh is activation function of h-th neuron of hidden layer
- αth is input signal of h-th neuron of hidden layer at time t
where:
- αtk is input signal of k-th neron of output layer at time t
- whk is connection weight between h-th neuron of hidden layer & k-th neuron of output layer
- bth is output layer of h-th neuron of hidden layer at time t
Gradient vanishing
In multi-layer neural network, first layers has slower
“learning speed" than last layers
■ Gradient vanishes when backpropagating from last layers to first layers
■ Similar problem happens to RNN in time dimension
■ Use LSTM cell to deal with problem
Gradient vanishing in RNN
Output layer
Hidden layer
Input layer
Time axis
Long-short Term Memory (LSTM)
Output gate ll
Forget gate
Input gate
Output layer
Hidden layer
Input layer
Time axis
Number of parameters of LSTM
I: Number of neurons in input layer
■ H: Number of LSTM cells (hidden layer)
■ K: Number of neurons in output layer
Number of parameters: 4 x (I x H + H x H + H) + H x K +
Number of connections between input & hidden layer: I x H
Number of recurrent connections in hidden layer: H x H
Number of connections between hidden & output layer: H x K
Number of biases of hidden layer: H
Number of biases of output layer: K
Bidirectional RNN
Ouput layer
Backwa rd layer
Forward layer
Input layer
RNN for sentiment analysis
Sentiment analysis:
Input: a document; xt is t-th word in input document
Output: sentiment of document (positive, neutral, negative)
Use output of last word xT for classifying: xT is considered representation of whole document(!)
Input layer has V neurons (V is dictionary size)
■ 1-hot encoding: Each word x in document corresponds to a word ti in dictionary & i-th neuron receives input signal equal to 1, other neurons receive input signal equal to 0
■ Output layer has 3 neurons as 3 sentiments output
Output layer
Hidden layer
Input layer
BPhon ver awe som phone
10. Ensemble learning
10.1 Bagging
Boostrap Aggregating)
■ Give a training dataset D has n observations & a learning algorithm
■ Training:
1. Randomly sample with replacement from D, repeat k times. We have k set of examples S1,
S2, …, Sk (n examples each). Si contains average 63.2% observation in D.
(lim n 🡪 ꝏ (1 – (1-1/n)^n) ~ 0.632)
2. Develop a classifier for each set Si using learning algorithm
Testing: Classify every example using k classifiers, final results determined by voting mechanism with symmetry coefficients
■ Bagging increases performance of unstable algorithm
(decision tree)
■ But bagging may decrease performance of stable algorithm (Naive Bayes, kNN)
10.2 Boosting
Training:
Create a sequence of classifiers (each classifier denpends on its previous)
Use same learning algorithm
Miss-classified examples will be adjusted weight, so that latter classifier can pay more attention on them
Testing: On each example, results of k classifiers (at each step) are summed by weighted voting
■ Boosting is suitable for unstable learning algorithms
Algorithm AdaBoost(D, Y, BaseLeaner, k)
Init D1(wi) ← 1/n for each i; // Initialize for t = 1 to k do ft ← BaseLearner(Dt);
// Create new classifier ft
// Computing error of ft threshold
10 if et > 1⁄2 then
// if error value is large than k ← k – 1; exit-loop; else αt if ft(Dt(xi)) αt ←=eyt i / (1 - et); nếu không
Dt+1(wi) ← Dt(wi) x
// update weights
// skip turn
// & exit
// adjust weights
14 endif endfor
// final classifier
Lesson 2:
MACHINE LEARNING
(CONT.)
Content
1. Basic Concepts
2. K-means algorithm
3. Cluster representation
4. Hierarchical clustering
5. Distance function
6. Normalize data
7. Handling multiple attribute types
8. Evaluation method
9. Explore holes & data areas
10. Learning LU
11. Learning PU
1. Basic Concept
Clustering is process of organizing data elements into groups in which members have similar properties.
Each cluster consists of data elements that are similar & different from data elements belonging to other groups
- Application: clustering customer groups based on interests to design marketing strategies; customer clustering based on body mass index to arrange clothing production; clustering articles to synthesize news; ...
2. k-means Algorithm
Algorithm k-means(k, D) select k data points as centroid (center of cluster) repeat for x ∈ D do calculate distance from x to each centroid; assign x to nearest centroid
// a centroid represents a cluster endfor recalculate centroids based on current clusters until stopping criterion is met
Convergence Condition:
1. number of reassigned data points is less than a threshold
2. number of centroids changed is less than a threshold
3. sum of squares of error is less than a threshold
- k: cluster number
- Cj : jth cluster
- mj is centroid of Cj (average vector of example in Cj)
- dist(x, mj) distance between x & mj
(A) Randomly select k centroid
Loop 1:
(B) Cluster assignment
Loop 2:
(D) Cluster assignment
Loop 3:
(F) Cluster assignment
(C) Recalculate centroid
(E) Recalculate centroid
(G) Recalculate centroid
Algorithm disk-k-means(k, D)
Choose k data point as centroid mj, j = 1, ..., k; repeat initialization sj ← 0, j = 1, ..., k; components equal to 0 initialization nj ← 0, j = 1, ..., k; in cluster j for x ∈ D do j ← argmin dist(x ,mi);
Assign x to cluster j; sj ← sj + x; nj ← nj + 1;
10 endfor
11 mj ← sj / nj, j = 1, ..., k;
12 until stopping condition is met
// 0 is a vector with
// nj is number of points
O(tkn) where t is number of iterations, k is number of clusters, n is number of examples in training data.
- Only apply to data where mean exists, for discrete data, apply k-modes algorithm
- Given k values
- Sensitive to outliers (points located far from rest of data set)
- Sensitization to initialization (often approaching local extremes)
- Not suitable for clusters with super sphere shape
+ outliers
Unwanted clustering
Ideal clustering outliers
(A) Random initialization
(B) Loop 1
(C) Loop 2
(A) Random initialization
(B) Loop 1
(C) Loop 2
(A) 2 clusters of natural super spheres
(A) Result of k-means (k = 2)
3. Cluster representation
Typical representation:
Centroid-based: Fits elliptical or spherical clusters
- Based on classification model: Assume each cluster corresponds to a class with members of cluster having corresponding class label
− Based on common values in cluster: Fits discrete values, including text
x ≥ 2 → cluster 1
X >2, y > 1.5 → cluster 2
X > 2, y ≤ 1.5 → cluster 3
4. Hierarchical clustering
 data is divided into a series of nested clusters in a tree structure
(dendrogram).
 leaves of tree are data points, root contains a single cluster, intermediate nodes contain subcluster nodes
Bottom Up Clustering: A pair of closest clusters at each level is pooled at next level. process repeats until only 1 cluster remains
Top-down clustering: An initial cluster containing all data. This cluster is divided into sub-clusters. A subcluster is recursively divided until only 1 element remains
Algorithm Agglomerative(D)
Consider each data point in D as a cluster,,
Calculate distance pairs of x1, x2, ..., xn ∈ D; repeat find 2 closest clusters; combine 2 clusters into a new cluster c; calculate distance from c to other clusters; until only 1 cluster remains
Single link method:
 distance between 2 clusters is shortest distance between 2 data points of each cluster
− Fits clusters without ellipses
− Can cause chaining effects due to noise in data
− Computational complexity: O(n2)
Full link method:
 distance between 2 clusters is maximum distance between 2 data points of each cluster
No chaining effects but sensitive to outliers
Computational complexity O(n2 log n)
Medium link method:
 distance between 2 clusters is average distance between 2 data points of each cluster
− Computational complexity: O(n2log n)
Chaining effect of single link method
Result of full link method
There are also other methods, for example:
 distance between 2 clusters is distance between their 2 centroids
- Ward method: distance between 2 clusters is increase in sum of squared errors from those 2 clusters to new cluster (if) combined
Advantages: Hierarchical clustering allows generation of clusters depending on level of tree
- Disadvantage:
Hierarchical clustering has high computational & storage costs
5. Distance function
5.1 Continuity attribute
Minkowski(xi, xj) =
Euclidean(xi, xj) =
Manhattan(xi, xj) =
Weighted_Euclidean(xi, xj) =
Squared_Euclidean(xi, xj) =
Chebychev(xi, xj) =
5.2 Binary & Discrete Properties
Data point xj a +c c+d b+d a+b
Data xi point a+b+c+d
Symmetric property: binary equal importance
Ambiguous matrix 2 of 2 datavalues pointsofcontaining only binary attribute dist(xi, xj) =
Eg: x1 x2 b+c a+b+c+d dist(x1, x2) =
Asymmetric attribute : 2 binary values of different importance b+c
Jaccard(xi, xj) = a+b+c
General discrete attribute : dist(xi, xj) = rqr
- r: number of attributes
- q: number of matches between xi & xj
6. Normalize data
Eg:
Attribute 1 has a value in range [0, 1], attribute 2 has a value in range [0, 1000] xi: (0.1, 20), xj : (0.9, 720)
After normalizing attribute 2 to interval [0, 1]
Xi: (0.1, 0.02), xj : (0.9, 0.72) → dist(xi, xj) =
Linear continuous attribute: xij rg(xif) = min(f) max(f) min(f) z(xif) = xij μf
Exponential continuum attributes: logarithmic
VD: AeBt
- Discrete attributes with no order (e.g. fruits): Can be converted to binary
- Ordered discrete attribute (e.g. age): similarly normalize linear continuous attribute
7. Handling multiple type attributes
Data contains many types of properties: symmetric binary, asymmetric binary, linear continuum, nonlinear continuum, discrete, ordered discrete
Convert all to most common property type (e.g. linear continuous)
Calculate distance on each attribute & sum it up r is number of attributes in data, dijf is distance between xi & xj in term of f, δijf = 1 if attribute f exists in both xi và xj & δijf = 0 otherwise
8. Evaluation method
User-Based: Based on a panel of experts. final rating is average of group. Fits some data type (text)
- Based on classification: Use classified data. Each class corresponds to a cluster. Using categorical measures
Compression metric: Shows concentration of data points in a cluster around centroid (e.g. sum squared error)
- Isolation metric: Expresses degree of separation of clusters through distance between centroids
- Indirect evaluation: Clustering is used as an intermediate task → evaluate clustering technique through evaluation of end task. E.g. user clustering is applied in recommender system
(product)
9. Exploring holes & data areas
 data has areas of concentration of data points & regions that contain no or little data (holes).
- discovery of data holes is important in a number of applications
- Classification problem:
1. Assume already existing data points have label Y. Add new data points randomly & label N
2. Use a classification technique (e.g. decision tree) to classify data
3. Obtain a classification model with interest label N
(A) Original data space
(B) Partitioning with additional data points
(C) Partition on original data
10. Learning LU
Supervised learning gives high accuracy but requires a lot of labeled data. Data labeling is manual work, requiring a lot of time & effort. In applications such as web document classification, data labels are constantly changing.
- LU learning (labeled & unlabeled examples) builds a classifier on a small amount of labeled data & uses a large amount of unlabeled data to improve classifier
- For example, a text classifier uses 'assignment' as a feature to classify texts on educational topics. Based on unlabeled data, it is possible to discover that 'assignment' often co-occurs with 'lecture', thereby adding 'lecture' to characterize classifier
10.1 EM algorithm
EM (Expectation Maximization) is an iterative algorithm to maximize probability estimate for missing data.
 expectation step fills in missing data based on estimate of current parameter. maximization step re-estimates parameter with goal of maximizing probability.
- EM+ classification model:
- 1) data labeled L is used to build classifier f
- 2) Use f to classify data that are not labeled U
- 3) Re-update f based on L & U; back 2), repeat until convergence
Algorithm EM(L, U)
Learning classifier NB f from dataset labeled L repeat
// Step E for each di in U do
Use f to calculate Pr(cj|di) endfor
// Step M learns classifier f from L & U (calculates Pr(cj) & Pr(wt|cj) until classifier parameters stabilize
Given Do include labeled examples, Du include unlabeled examples
 log likelihood function has form:
Instead of maximizing log likelihood, we maximize expectation of full log likelihood:
Conditional probability of a document knowing its class:
Assuming texts are generated independently, likelihood function can be written as:
Form log likelihood function:
Use indicator hki, hki = 1 when document i has label k, form log likelihood:
Expectations of full log likelihood
Lagrangian:
Taking derivative with respect to λ, we have
Taking derivative with respect toPr(ck; Θ): với k = 1, 2, …, |C|
Sum by k, we have:
Update Pr(cj| Θ):
Update Pr(wt| cj, Θ):
10.2 Co-training
Suppose set of attributes can be divided into 2 sets X1 & X2 to build 2 classifiers f1 & f2
- Assumption
1: classifier built on entire attribute f has same classification result as f1 & f2
- Assumption
2: X1 & X2 are independent of each other for class labels
Algorithm co-training(L, U) repeat
Build classifier f1 using L based on attribute set X1
Build classifier f2 using L based on attribute set X2
Use f1 to classify examples in U, for each class ci, choose ni examples f1 is most confident in & add L.
Use f2 to classify examples in U, for each class ci, choose ni examples f2 is most confident in & add L. until U is empty (or after a certain number of loops)
(A) Data classified by f1 over U
(B) Additional data by f1 for f2
10.3 Self - training
 classifier f is built on set L
- classifier f is used to classify examples in set
- examples with highest confidence are added to L
- process repeats until end (e.g. U is empty)
10.4 Inference on SVM
Hyperplane selection for margin maximization based on unlabeled examples y=1
Old hyperplane y = -1
New hyperplane
10.5 Graph-based methods
From L & U, construct a graph of vertices as examples & weighted edges as similarity between examples.
From a vertex, choose k nearest neighbors
- Select similarity above a minimum threshold
- Use a fully connected graph with exponential similarity
Label examples in U based on labeled examples in L such that similar vertices have same label
Mincut: Labeled vertices belonging to L are assigned value {0,1} depending on positive or negative class; weighted edges wij; find a way to label unlabelled vertices of U such that Σ(i,j) ∈ E wij|vi - vj| smallest
- Find division of vertex set V into 2 nonintersecting sets V+ và V- such that sum of weights of edges joining 2 sets is minimal. V+ contains positively labeled vertices of L & vertices of U; Vcontains negative labeled vertices in L & vertices in U
- Maximum flow algorithm with
O(|V|3) omputational complexity
Gaussian field: Minimizing Σ(i,j) ∈ E wij|vi – vj|2 with values in [0, 1]
- Spectral graph: Minimize cut (V , V ) / (|V ||V |) o balance number of vertices of 2 sets because mincuts tend to choose 2 unbalanced sets
11. Learning PU
In some applications user is only interested in 1 layer of text (positive) & not in other text
(negative).
- Given set P consisting of positive documents & set U consisting of unlabeled documents
(including positive & negative texts), it is necessary to build a classifier that allows identifying positive documents.
11.1 Applications of learning PU
For example: It is necessary to build a database of scientific works in field of data mining.
First, use works of conferences & journals on data mining as positive documents
Next, find works on data mining in conferences & journals on databases & artificial intelligence
Learn with a variety of unlabelled data sources: e.g., find printer websites
Find positive pages from amazon.com
Use PU learning to find positive pages in other sites
Learning with negative data is not reliable
Expanding dataset
Covariate shift
11.2 Theoretical background
Classification based only on positive examples
+ + o + o o
+ + + o o oo o o o
Classification based on positive & negative example
(xi, yi) are random variables taken from probability distribution D(xi, yi), y ∈ {1, -1} is a conditional random variable that we need to estimate when we know x, Dx|y = 1 is conditional distribution on which positive examples are generated, Dx is marginal distribution that produces unlabeled examples.
- objective is to build a classifier f để phân loại văn bản positive và negative, to classify positive & negative documents, classifier with smallest probability of error generation Pr(f(x) ≠ y)
Pr(f(x) ≠ y) = Pr(f(x) = 1 và y = -1) + Pr(f(x) = -1 và y = 1)
We have:
Pr(f(x) = 1 và y = -1)
= Pr(f(x) = 1) - Pr(f(x) = 1 và y = 1)
= Pr(f(x) = 1) - (Pr(y = 1) - Pr(f(x) = -1 và y = 1))
Pr(f(x) ≠ y) = Pr(f(x) = 1) – Pr(y = 1) + 2Pr(f(x) = -1| y = 1)Pr(y = 1)
Since Pr(y = 1) = const, minimize:
Pr(f(x) = 1) + 2Pr(f(x) = -1| y = 1)Pr(y = 1) if Pr(f(x) = -1| y = 1) <<, approximation to minimize Pr(f(x) = 1) approximation to minimize PrU(f(x) = 1) with PrP(f(x) = 1) ≥ r (recall)
optimal classifier o o oo o o o
+ + o + o o
11.3 Classifier construction: 2 - steps
Step 1: Determine set of reliable negative texts RN from U
Step 2: Build a classifier from P, RN & U-RN
Step 1
Positiv
Negativ
Step 2
RN
Q = U - RN
Bước 1:
- Spying Techniques:
1. Randomly select a set S from P & put it in U (line 2-3).
Texts in S allow to find positive texts in U
2. Build a classifier NB with PS as positive set & Us as negative set (line 3-7). Use classifier NB to determine probabilities Pr(1|d) for documents in Us
3. Use probability Pr(1|d) with texts in S to find threshold t. Texts with probability Pr(1|d) < t are included in set RN (line 10-14).
Algorithm spy(P, U)
RN ← ∅;
S ← sample(P, s%);
Us ← U ⋃ S;
Ps ← P – S;
Assign each text in Ps to layer 1;
Assign each text in Us to layer -1;
NB(Us, Ps);
// build NB model classifies documents in Us using NB classifier;
Determine probability threshold t based on S;
10 for d ∈ Us do
11 if probability Pr(1|d) < t then
RN ← RN ⋃ {d};
13 endif
14 endfor
Cosine-Rocchio Technique:
- Step 1: Determine potential negative set PN
Representing documents into vectors according to TF-IDF
- Construct feature vector v
P for set P. Calculate cosine similarity between documents in P with vP & arrange them in descending order.
- A threshold is defined to get all positive texts hidden in
U & get minimum of negative texts.
Step 2: Build Rocchio f classifier based on P &
PN. documents in U are classified as negative because f is included in set RN
Algorithm CR(P, U)
PN = ∅; RN = ∅;
Represent each document d ∈ P & U as vector TF-IDF;
Calculate cos(vp, d) for each document d ∈ P;
Sort all texts d ∈ P based on cos(vp, d) in descending order; ω = cos(vp, d) where d is placed at (1- l)*|P|; for d ∈ U do if cos(vp, d) < ω then
PN = PN ∪ {d}
14 for d ∈ U do if cos(cPN, d) > cos(cP, d) then
RN = RN ∪ {d}
1DNF technique: Word in P & U are collected to build vocabulary (line 1). built positive PF feature set contains words occurs that is more common in P than U (line 2-7). Documents in U that contain no attributes in PF are included in RN set
(line 8-13).
- NB technique: Use an NB classifier to build a set of
RNs from U
- Rocchio Technique: Using a Rocchio classifier to build RN sets from U
Algorithm 1DNF(P, U)
Suppose vocabulary set V = {w1, ..., wn}, wi ∈ U ∪ P;
Initialize positive attribute set PF ← ∅; for each wi ∈ V do
// freq(wi, P): number times wi P if (freq(wi, P) / |P| > freq(wi, U) / |U|) then
PF ← PF ∪ {wi}; endif endfor
RN ← U; for each document d ∈ U do
10 if exists wj such that freq(wj, d) > 0 & wj ∈ PF then
RN ← RN – {d}
12 endif
13 endfor
Algorithm NB(P, U)
Assign text in P: class label 1;
Assign text in U: class label -1;
Build classifier NB using P & U;
Use classifier to classify documents in U. Texts are classified negative type is included in set RN
Bước 2:
- EM
+ NB: expectation step calculates label probabilities of documents in U – RN. maximization step re-estimates classifier parameter
NB
- SVM: At each iteration, SVM classifier f is built from P & RN. classifier f is used to classify documents in
Q. Texts classified as negative are removed from Q & added to RN. iteration ends when there are no more documents in Q that are classified as negative.
Algorithm EM(P, U, RN)
Each text in P is assigned a class label: 1;
Each text in RN is assigned a class label: -1;
Learning a classifier NB f from P & RN repeat for each di in U - RN do
Use current f classifier to calculate Pr(cj| di) endfor learns new classifier NB: f from P, RN & U - RN by computing
Pr(cj) & Pr(wt| cj) until classifier parameters are stable
Algorithm I-SVM(P, RN, Q)
Each document in P is assigned a class label 1;
Each document in RN is assigned a class: –1; loop
Use P & RN to train SVM classifier f;
Classify Q use f;
W is set of documents in Q that are classified as negative; if W = ∅ then exit-loop
// converge else Q ← Q – W;
RN ← RN ∪ W;
10 endif
11.4 Create classifier: SVM
Given train dataset {(x1, y1), (x2, y2)…, (xn, yn)} where xi is vectors & yi ∈ {1, -1}. Assuming first k examples
∈ P have a positive label (y = 1), following examples
∈ U have a negative label (y = -1)
- TH1: Set P does not contain errors, theoretically when number of samples is large enough, a good enough classifier can be obtained if unlabeled documents are minimized to be classified as positive while positive example constraint is good classification
Minimize
<w ⋅ xi> + b ≥ 1, i = 1, 2, …, k-1
-1(<w ⋅ xi> + b)
≥ 1 - ξi, i = k, k+1, … n ξi ≥ 0, i = k, k+1,
…, n constraints :
TH2: set P contains a negative example (due to noise in data)
Minimize:
Constraint: yi(<w ⋅ xi> + b) ≥ 1 - ξi, i = k, k+1, … ξi ≥ 0, i = k,
Where C+ & C- are weights of positive & negative errors, respectively, k+1, …, n determined based on validation set by measure
Pr(f(x) =
r = Pr(f(x) = 1| y = 1) p = Pr(y = 1| f(x) = 1)
We have:
Pr(f(x) = 1| y = 1)Pr(y = 1) = Pr(y = 1| f(x) = 1) Pr(f(x) = 1)
Pr(f(x) = 1
Pr(y = 1 )
Pr(f(x) = 1
Pr(yr = 1 )
Similar to F-score
11.5 Classifier Construction:
Probability Estimation
Example x & label y ∈ {1, -1}, s = 1 if x is labeled & s = 0 if x is unlabel, we have
Pr(s = 1| x, y = -1) = 0
- Objective: Build classification function f(x) such that f(x) is closest to Pr(y = 1| x)
- Totally Random Selection Hypothesis: labeled positive examples are chosen completely randomly from positive examples Pr(s = 1| x, y = 1) = Pr(s = 1| y = 1) = c
- If P & U are used to build classification function g(x) then g(x) = Pr(s = 1| x), then f(x) = g(x) / c
g(x)
= Pr(s = 1| x)
= Pr(y = 1 và s = 1| x)
= Pr(y = 1| x)Pr(s = 1| y = 1, x)
= Pr(y = 1| x)Pr(s = 1| y = 1)
= f(x)Pr(s = 1| y = 1)
Estimate c using validation set V, where VP là tập các ví dụ positive trong V: g(x) = Pr(s = 1| x)
= Pr(s = 1| x, y = 1)Pr(y = 1| x) + Pr(s = 1| x, y = -1)Pr(y = -1| x)
= Pr(s = 1| x, y = 1) × 1 + 0 × 0 do x ∈ VP
= Pr(s = 1| y = 1).
Note: If it is only necessary to rank x according to probability of being in positive class, g . can be used directly
LESSON 3: DATA
VISUALIZATION
Content
1. Static chart
2. Pixel-based visualization
3. Visualization in vector space
4. Super sphere tree
5. SOM
1. Static chart
1.1 Attributes
Data objects represent entities in data (e.g. customers, products, transactions)
■ Data objects are also known as samples, examples, or data points
■ An attribute is a data field that represents a property or features of data
■ Attributes are also known as dimensions, features, or variables
 values of a given attribute are called observations
■ set of attributes that describe a given object is called an attribute vector (or feature vector).
■ attribute type is determined by set of attribute values
Nominal attribute
Valuable as symbols or names
■ eg: 'hair color' includes 'blue', 'red', 'black', 'white',
'platinum'
■ Description of categories, codes, states
■ Common value based on mode function
Binary attribute
 category attribute has only 2 categories or 2 states
0 ~ absent, 1 ~ exists
■ or 0 ~ false, 1 ~ true
Symmetrical attributes (e.g. 'gender' includes 'male' & 'female')
■ Asymmetric attributes (e.g. 'result' includes 'positive' & 'negative'
Ordinal attribute
 values follow a certain order
■ Example: 'size' includes 'small', 'normal', 'large' &
'oversized'
■ Typical value based on mode & median function
Interval attribute
Use to define values measured along a scale, with each point placed at an equal distance from 1 another.
■ Can compare, calculate distance between values
■ For example: Temperature according to Celsius scale.
Ratio atribute
Thuộc tính số có giá trị 0
■ Có thể nhân các giá trị với nhau
■ VD: Các giá trị đếm và đo đạc:
Số lượng
■ Trọng lượng
■ Chiều cao
■ Số tiền
Discrete & Continuous Attributes
Has only a finite or countably infinite set of values.
■ Example:
Finite: color, age
■ Countable infinite: Customer ID
Attribute is continuous if not discrete
1.2 Basic data statistics
Data Description:
Central value
■ Distribution range
■ Visualization based on charts
Identify outliers
mean
Values have same role x=
Values with different weights x= x1 + x2 + …
+xn w1x1 + w2x2 + …
+wnxn
 most common measurement, however sensitive to outliers
median
 median divides data into larger & smaller parts; These 2 parts have same number of elements
■ Calculate median approximation
Group data into ranges of values
■ Calculate frequency of values in each interval
■ Find interval containing median frequency
median
Approximate median by formula: median = L1 +
N/2 (Σfreq)l freqmedia widt
Where:
- L1: lower boundary of median
- N : number of value
- (Σfreq)l sum of frequencies of intervals less than median
- freqmedian: frequency of median range
- width: width of median range
mode
 most frequency value in dataset
■ Multimodal: more than 1 mode
■ Set containing only unique values without mode
■ Với tập unimodal (1 mode) mean - mode ≈ 3 x (mean - median)
midrange
Average of maximum & minimum values max + min midrange =
range distance between largest & smallest value in set range = max - min
quantile
Quantiles are points that divide data into (nearly) equal parts (with equal number of elements).
2-quantile: a point that divides data into 2 equal parts ~ median
■ 4-quantile (quartile)
■ 100-quantile (percentile)
■ Interquartile range IQR = Q3 Q1
Interquartile range IQR = Q3
- Q1
boxplot
 box chart includes:
Q1, Q3: beginning & end of box
■ IQR: length of box
■ Median
■ Min & max values max media
IQR mi
variance, standard deviation
Variance σ: standard deviation represents dispersion of data relative to mean
Quantile chart
Sort values in ascending order x1 < x2 <
… xn
■ Frequency f corresponding to xi là is percentage of data with values below xi i – 0.5 fi =
Quantile - quantile chart
Show relationship between quantile values of 2 univariate distributions
Histogram
Values are grouped into equal intervals called buckets.
Scatter chart
Determine reciprocity between 2 numeric attributes
Scatter chart (cont.) positive reciprocity negative reciprocity
Null reciprocity
2. Pixel based visualization
 value of a data dimension represented by a pixel with color corresponding to value
■ Example: Small value corresponds to light color, large value corresponds to dark color
■ m dimension corresponds to m window. A data point with m dimensions is represented by m pixels at corresponding positions in each window
Records are usually sorted in a dimension of interest
■ Correlation, if any, between data dimensions is expressed through color distribution (data values) over windows
3. Visualization in vector space
Pixel-based visualization does not represent density of data points
■ Visualization on vector space based on projection technique to represent multidimensional data in 2D space
Scatter Charts
How to represent:
2 axes X & Y used to represent 2-dimensional numbers in Cartesian coordinates
■ third dimension is represented by different shapes
■ fourth dimension can be represented by color
Scatter Chart (cont) www.cs.sfu.ca/jpei/publications/rareevent-geoinformatica06.pd
Scatter chart matrix
Scatter charts can only represent up to 4 dimensions
■ For data with more than 4 dimensions, use scatter plot method
Data has m dimensions
■ Use a 2D scatter plot m x m matrix to represent each data dimension with remaining data dimensions
■ Example: Iris dataset has 5 dimensions visualized by a
4 x 4 matrix consisting of 24 3D scatter plots
Example: Iris Dataset http://support.sas.com/documentation/cdl/en/grstatproc/61948/HTML/default/images/gsgscmat.gif
4. Super sphere tree
Visualize large amounts of data
■ Data has a tree structure
■ focus on a part of data, on other hand still represent general context of data
■ Fish eye properties:
 size of unfocused buttons rapidly decreases when
■ size of buttons are focused is rapidly increasing
Example
5. SOM (Self Organizing Map)
SOM learns 2D representation of multidimensional data
■ SOM is a feed-forward neural network (2 layers)
Input layer receives a signal from input data, whose dimension is equal to dimension of data
■ Competition layer is organized in a certain shape (rectangle, hexagon, etc.) showing spatial relationship between neurons.
■ Each neuron in competition layer has association weights from input layer called a reference vector
SOM architecture
Input layer competition layer reference vector
Wnx
Competitive learning
For each input xi, select neuron mk whose distance to xi is smallest
■ Distance between x & m : euclide distance between xi & reference vector of mk
■ Objective function: Minimize total distance between input & corresponding nearest neuron
■ Weight Update: Only update weights of m & neighboring neurons
Example: neighboring neurons
Competition layer neuron with minimum distance to xi
WEBSOM
Representation of a set of documents on a 2D map
■ Texts are represented as bags of words
■ After learning, each text group can be represented by specific keywords
■ Areas with high density are places where a lot of text is concentrated
WEBSOM
Lesson 4:
Information Retrieval
Agenda
1. Basic Concepts of Information Retrieval
2. Information Retrieval Models
3. Relevance Feedback
4. Evaluation Measures
5. Text & Web Page Pre-Processing
6. Inverted Index & Its Compression
7. Latent Semantic Indexing
8. Web Search
9. Meta-Search: Combining Multiple Rankings
10. Web Spamming
1. Basic Concepts of Information Retrieval
Information retrieval (IR) helping users to find information that matches their information needs
IR studies acquisition, organization, storage, retrieval & distribution of information.
Hệ thống tìm kiếm thông tin truyền thống coi văn bản là đơn vị cơ bản
IR is about document retrieval, emphasizing document as basic unit.
User issues a query (user query) to retrieval system through query operations module. retrieval module uses document index to retrieve documents relevant to query
(contain some query terms), compute relevance scores for them & then rank retrieved documents according to scores.
 ranked documents are then presented to user. document collection is also called text database, which is indexed by indexer for efficient retrieval.
User
User feedback
User query
Query operations
Ranked documents
Document collection
Indexer
Executable query
Retrieval
System
Document index
Type of user query
1. Keyword query: a list of keywords (or terms) to find documents contain some or all query terms. Order of keywords is also significan & affect results. E.g: information retrieval
2. Boolean query: consists of terms & Boolean operators &, OR & NOT. E.g: information OR retrieval
3. Phrase query: consists of a sequence of words that makes up a phrase. Each returned document must contain at least 1 instance of phrase. E.g: “information retrieval systems"
4. Proximity query: can consists combination of terms & phrases.
Returned documents are ranked by distance between terms & phrases.
5. Full document query: find documents that are similar to query document
6. Natural language question (question answering): most complex case. User express query as natural language questions.
Query operations can range from very simple to very complex: remove stopwords, transform natural language queries into executable queries, use user feedback to expand & redefine original query.
Indexer index original raw documents in some data structures to enable efficient retrieval. result is document index.
Invert index is easy to build & very efficient to search.
Retrieval system computes a relevance score for each indexed document to query. documents are ranked & presented to user. Only documents that contains at least 1 query term is first found from index are scored.
2. Information Retrieval Models
Information retrieval models represent documents & queryies & score relevance of a document to a user query.
- Documents & queries are represented as bag of words. Sequence & position are ignored.
- Collection of documents D
- V = {t ,t ,…,t
|V|} is set of distinct terms in D, ti is a term. V is usually called vocabulary & |V| is vocabulary size.
- Each term t in document d ∈ D has w important weight ij of ti in dj; dj = (w1j,w2j,…,w|V|j)
2.1 Boolean Model
Document representation: documents & queries are represented as sets of terms.
1 if ti in dj wij =
0 otherwise
Boolean query: terms are combined with Boolean operators &, OR và NOT. E.g:
((x & y) & (NOT z)) return document contain both x & y but not z.
(x OR y) return document contain at least on term x or y
Document retrieval: All document satisfied query are returned without ranking
2.2 Vector Space Model
Document representation:
Tf: number of times ti appears in document dj (fij).
Cons: does not consider situation where a term appears in many documents of collection. tfij= max(f ,fij ,…,f )
1j 2j
|V|j
Tf-idf: terms that appear in many documents have low weights idfi=log dfi wij = tfij x idfi
N: number of documents; dfi number of documents have ti
Query (Salton & Buckley)
0.5fij wiq= 0.5 + max(f ,f ,…,f ) x log df
1q 2q
|V|q
Ranking: Base on similarity between document dj & query
Cosine similarity is most well known.
Okapi is more efficient on for short query retrieval dlj is document length of dj (bytes) avdl is average document length of collection
2.3 Statistical Language Model
Estimate language model for each document, then ranks document by likelihood of query given language model.
- Query q is a sequence of terms q=q q ...q , document
1 2 collection D = {d1,d2,…,dN}. Estimate posterior probability,which is probability of query q is being generated from a probabilistic model based on a document dj: Pr(q|dj)
Pr(dj|q) =
Pr(q|dj)Pr(dj)
Pr(q)
Unigram language models assume each individual term is generated independently by multinomial distribution over words. where fiq number of time ti appears in q &
Pr(ti|dj) is estimated by: where |dj| number of words dj
Smoothing: prevent zero probabilities (for unseen term in document) ( λ=1 Laplace smoothing)
3. Relevance feedback
User identifies some relevant & irrelevant documents in initial list of retrieved documents & system then creates an expanded query by extracting some additional terms from sample relevant & irrelevant documents for a second round of retrieval.
 system may also produce a classification model using useridentified relevant & irrelevant documents to classify documents in document collection into relevant & irrelevant documents.
 relevance feedback process may be repeated until user is satisfied with retrieved result.
3.1 Rocchio Method
Query q, set of relevant documents selected by user Dr, set of irrelevant documents Dir, expanded query qe:
Original query q is augmented with additional terms from relevant documents Dr
- Irrelevant documents D ir are used to reduve influence of discriminative terms (appear in both or only in Dir)
3.2 Machine Learning Method
Vector ci is built for each class i, which is relevant & irrelevant (negative elements or components of ci is usually set to 0)
Each test document dt is compared with everyvector ci based on cosine similarity. dt is assigned to class with highest similarity value for each class i do build vector ci for each class dt do class(dt) = argmaxi cosine(dt,ci)
Learning from Labeled & Unlabeled examples (LU
Learning): number of user-selected relevant & irrelevant documents is small. Unlabeled examples, those are not selected by user, can be utilized to produce a more accurate classifier.
Learning from Positive & Unlabeled examples (PU
Learning): In some cases (like web search) user only select relevant documents (based on title or summary). These documents are called positive examples. unselected are unlabeled examples (implicit feedback)
Ranking SVM: use selected document to rank unselected documents
Language models
? Any other?
3.3 Pseudo-Relevance Feedback
Extract some terms (usually most frequent terms) from top-ranked document to original query to form a new query for second round of retrieval
- process can be repeated until user satisfied with final results
- Assume that top-ranked documents are relevant & user is not involved in process
4. Evaluation Measures
Rq: <d1q,d2q,…,dNq> is rank of document based on relevance score
- D is set of relevant document
- Recall at rank position i r(i) = si
|Dq| where si is number of relevant document from d1q to diq
- Precision at rank position i p(i) = si
Average precision
position relevant
8/8 p(i) r(i) position relevant p(i) r(i)
8/8 pavg =
i p(ri) ri
Precision – Recall Curve
Evaluation using multiple queries where Q is set of queries và pj(ri) is precision of query j at recall level ri
Although in theory precision & recall do not depend on each other, in practice a high recall is almost always achieved at expense of precision & a high precision is achieved at expense of recall.
In many applications, it can be very hard to determine set of relevant documents Dq for each query q. (on Web, Dq is almost impossible)
Rank precision: P(5), P(10), P(15), P(20), P(25), P(30)
F-score
F(i) =
2p(i)r(i) p(i)+r(i)
5. Text & Web Page preprocessing
Text preprocessing:
Remove stopwords
Stemming
Digits, hyphens, punctuations & cases of letter
Web Page preprocessing
HTML tag
Identify main content
5.1 Remove stopwords
Stopwords frequently occurring & insignificant words that help construct sentences but do not represent any content of documents
Articles, prepositions & conjunctions & some pronouns are stopwords a, about, an, are, as, at, be, by, for, from, how, in, is, of, on, or, that, , these, this, to, was, what, when, where, who, will, with,...
Such words should be removed before documents are indexed & stored.
Stopwords in query are also removed before retrieval is performed
5.2 Stemming
In many languages, a word has various syntactical forms depending on contexts. For example, in English, nouns have plural forms, verbs have gerund forms (by adding “ing") & verbs used in past tense are different from present tense. These are considered as syntactic variations of same root form.
Cause low recall for a retrieval system because a relevant document may contain a variation of a query word but not exact word itself.
Stemming: reducing words to their stems or roots. In English, most variants of a word are generated by introduction of suffixes (rather than prefixes).
Thus, stemming in English usually means suffix removal, or stripping. For example, “computer", “computing" & “compute" are reduced to “comput".
“walks", “walking" & “walker" are reduced to “walk"
Martin Porter's stemming algorithm use a set of rules
Cons of stemming: could return irrelevant document, for example, both “cop" & “cope" are reduced to stem “cop".
5.3 Other Preprocessing Tasks for Text
Digits: In traditional IR systems, digits are removed except some specific type, e.g., date, time. However, in search engines, they are usually indexed.
- Hyphens: system usually follow a general rule with exceptions. There are 2 type of removal:
Replace hyphens by space, e.g., 'pre-processing' → 'pre processing'
Delete hyphens, e.g., 'state-of--art'→ 'stateoftheart'
Punctuation marks: similar to hyphens
- Case of letter: Convert into 1 case
5.4 Web Page Preprocessing
Identify different text fields: HTML has ,many text fields, e.g., title, metadata & body. In search engines terms that appear in title field of a page are regarded as more important than terms that appear in other fields & are assigned higher weights because title is usually a concise description of page. In body text, those emphasized terms (e.g., under header tags <h1>, <h2>, ..., bold tag <b>, etc.) are also given higher weights.
Identifying anchor text: Anchor text associated with a hyperlink is treated specially in search engines because anchor text often represents a more accurate description of information contained in page pointed to by its link
Removing HTML tags: removal of HTML tags can be dealt with similarly to punctuation. 1 issue needs careful consideration, which affects proximity queries & phrase queries
Identifying main content blocks: A typical Web page, especially a commercial page, contains a large amount of information that is not part of main content of page
(banner ads, navigation bars, copyright notices)
Partitioning based on visual cues: X & Y coordinates of each element
Tree matching: pages are generated by using some fixed templates. method thus aims to find such hidden templates. Since HTML has a nested structure, it is thus easy to build a tag tree for each page.
5.5 Duplicate detection
Duplicate are often used to improve efficiency of browsing & file downloading worldwide to tackle network problems.
Some duplicate pages are results of plagiarism.
Detecting such pages & sites can reduce index size & improve search results
Copying a page is usually called duplication or replication & copying an entire site is called mirroring
Hashing or checksum can detect exact duplicate.
To detect partial duplicate, use technique based on n-gram
(or shingles) & Jaccard similarity (e.g., sentence,
“John went to school with his brother," can be represented with five 3-gram phrases “John went to", “went to school",
“to school with", “school with his" & “with his brother")
|S (d ) ∩ S (d )| sim(d1,d2) = |Sn(d1) U Sn(d2)| n 1 n 2
6. Inverted Index
Baseline: for each query, scan every document in database to find terms in query → not efficient
- Inverted index boost speed of searching & database building
6.1 Inverted Index
Inverted index of a set of documents is a data structure that attacks each term with a list of documents contain that term.
Searching time of term in document is constant to number of documents
Set of documents D={d1,d2,…,dN} each document has a unique id; inverted index of D contains:
Vocabulary V contain all distinct term in all documents
Each term ti is attack with a list of postings, each posting stores id of a document contain ti & some information:
<idj, fij, [o1,o2,…,o|fij|]> idj is unique id of dj fij frequency of ti in dj ok is offset of ti in dj
Postings list is sort by id & & offset
id1: Web mining is useful.
3 4 id2: Usage mining applications. id3: Web structure mining studies Web hyperlink structure.
V={web,mining,useful,applications,usage,structure,studies,hyperlink} applications: id2 applications: <id2,1,[3]> hyperlink: id3 hyperlink: <id3,1,[7]> mining: id1, id2, id3 mining:
<id1,1,[2]>, < id2,1,[2]>, <id3,1[2]> structure: id3 structure: <id3,2,[2,8]> studies: id3 studies:
<id3,1,[4]> usage: id2 usage:
<id2,1,[1]> useful: id1 useful:
<id1,1,[4]> web: id1, id3 web:
<id1,1,[1]>, <id3,2,[1,6]> a) b)
6.2 Search using Inverted Index
Given query terms, searching for relevant documents in inverted index consists of 3 main steps:
1. Vocabulary search: find each query term in vocabulary, which gives inverted list of each item. To speed up, vocabulary is usually strored using specicial data structure like hash, tries, B-tree.
Lexicographical order can be used due to its space efficiency.
Binary search can be applied. complexity is O(log|V|) where |V| size of vocabulary
2. Results merging: Traverse all lists in synchronization to check whether each document contains all query terms. Start with shortest list, for each document in it, binary search on other lists to find same document. whole inverted index can not fit in memory, so part of it is cache in memory for efficient.
3. Rank score computation: compute rank score for each document base on a relevance function (e.g. cosine or okapi), my consider term & phrase proximity information.
Example, query “web mining":
Step 1: found 2 inverted index mining: web:
<id1,1,[2]>, < id2,1,[2]>, <id3,1[2]>
<id1,1,[1]>, <id3,2,[1,6]>
Step 2: found id1 & id3 contains both 2 term
Step 3: id1 has higher relevance score than id3 due to “web" &
“mining" come together in id1 & has same order as in q
6.3 Index construction
Trie, a data structure, has complexity of O(T), where T is size of vocabulary for each doc do for each term in doc do if term in trie update inverted list else add term into trie
'a' applications: <id2,1,[3]>
'h'
'm' hyperlink: <id3,1,[7]> mining: <id1,1,[2]>, < id2,1,[2]>, <id3,1[2]>
's'
'r' structure: <id3,2,[2,8]>
't'
'u'
'u' studies: <id3,1,[4]> usage: <id2,1,[1]>
'a'
'w'
's'
'e'useful: <id ,1,[4]> web: <id1,1,[1]>, <id3,2,[1,6]>
Problem: not enough memory to save all index → solution:
Partial build index in memory & save to disks, I1, I2,…,Ik
Pair-wise merge until all index are merge into 1, I1, I2 into I1-2,
I3, I4 thành I3-4
Problem: pages are constantly added, modified or deleted → Solution:
Build index D+ for added pages & D- for deleted pages
Search on all of indexes, final results: D U D+ \ D-
6.4 Index compression
Index compression is for memory size reduction without information lost
Information is represented with positive integers → integer compression technique
In variable-bit (also called bitwise) scheme, an integer is represented with an integral number of bits. (unary coding,
Elias gamma coding, delta coding, Golumb coding)
Variable-byte use 7 bits to represent value & very right bit as delimiter (0 if it is last byte, otherwise 1)
Since ID are sorted in increasing order, we can store smallest ID & difference between any 2 adjacent document IDs.
 gap is smaller than ID value & thus requires fewer bits
Unary coding: represent x by x-1 bit 0 & a bti 1, e.g. 5 = 00001
Elias gamma coding: Represent 1+[log2x] in unary follow by binary representation of x without most significant bit. Efficient for small integers.
E.g.: 9: 0001001 since 1+[log29]=4 & 9=1001(2)
Decode:
1. Read K 0 until counter 1;
2. Consider 1 reached is first digit of integer (2K), read next K bits.
To decode 0001001, we have K=3 bit 0, thus binary representation is 1001(2)=9
Elias delta coding: Represent 1+[log2x] in gamma follow by binary representation of x without most significant bit. Efficient for large integers
E.g.: 9=00100001 since 1+[log29]=4 has gamma coded of 00100
Decode:
1. Read L 0 until reach first 1
2. Consider reached 1 is first bit of integer (2L), read next L bit to get integer M.
3. Put 1 to first place of final output (2M) & read next M-1 bit. e.g.: decode 00100001, 1. L=2; 2. M=4; 3. 1001(2)=9
Golomb coding:
1. First part is unary representation of q+1 where q=⌊(x/b)⌋
2. Next part is binary representation of remain r=x-qb. consider i = ⌊log2b⌋, d=2i+1-b , first d remainders are represented using i bit; others represented using ⌈log2b⌉
(i+1) bit (fixed prefix coding) e.g.: x=9, b=3; q=⌊(9/3)⌋=3, i=⌊log23⌋=1, d=21+1-3=1, r=9-3x3=0 → 9: 00010 select b: b ≈ 0.69 nt where N is number of documents, nt number of documents contain t
Decode:
1. decode q
2. i = ⌊log2b⌋, d = 2i+1-b
3. Retrieve next i bits & assign it to r.
4. If r ≥ d
Retrieve 1 more bit & append to r at end r=r-d
Cây mã hóa với b=5
5. x = qb + r
E.g: decode 11111 where b = 10
1. q = 0 2. i = ⌊log210⌋ = 3, d = 23+1 – 10 = 6 3. r = 111(2) = 7 4. 7
> 6→ r = 1111(2) = 15; r = 15 – 6 = 9 5. x = 0x10+9=9
Variavle-byte coding: 7 bits to represent, very right bit = 0 if last byte, otherwise 1. efficient for small integers. E.g. 135: 00000011
Decode:
1. Read all bytes until a byte with zero last bit is seen
2. Remove least significant bit from each byte read so far & concatenate remaining bits. e.g.: 00000011 00001110 decode to 00000010000111(2) = 135
Comment:
Golumb coding has better compression ratio & faster retrieval than nonparameterized Elias coding
Variable-byte are often faster tham variable-bit integers despite having higher storage costs
A suitable compression technique can allow retrieval to be up to twice as fast than without compression, while space requirement averages 20% –
25% of cost of storing uncompressed integers
7. Latent Semantic Indexing
If a user query uses different words from words used in a document, document will not be retrieved although it may be relevant because document uses some synonyms of words in user query. For example, “picture", “image" & “photo" are synonyms in context of digital cameras.
Lantent Semantic Indexing (LSI) aims to deal with this problem through identification of statistical associations of terms. It is assumed that there is some underlying latent semantic structure in data that is partially obscured by randomness of word choice.
LSI use Singular Value Decomposition (SVD) to estimate this latent structure & to remove “noise"
Transformed terms & documents in “concept" space are then used in retrieval, not original terms or documents.
 query is also transformed into “concept" space before retrieval
7.1 SVD
SVD fector a matrix m x n A into product of 3 matrix
A=UΣVT where
U is a m x r matrix & its columns called left singular vectors, are eigenvectors associated with r non-zero eigenvalues of AT;
UTU=I
V is an n x r matrix & its columns, called right singular vectors, are eigenvectors associated with r non-zero eigenvalues of ATA;
VTV=I
Σ is a r x r diagonal matrix, Σ=diag(σ1, σ2, …, σr), σi > 0. σ1, σ2, …, σr (called singular values) are non-negative square roots of r (non-zero) eigenvalues of AAT. σ1 ≥ σ2 ≥ … ≥ σr > 0 m: number of terms; n: number of documents; r rank of A A, r ≤ min(m, n)
Ak=UkΣkVkT
Use k-largest singular triplets to approximate original term-document matrix A (Ak), new space is called k-concept space, remove “noise" document term vector
Σk term
A/Ak mxn
Uk mxr
Vk T document vector
VT rxr rxn
SVD rotates axes of mdimensional space of A such that first axis runs along largest variation (variance) among documents, second axis runs along second largest variation (variance) & so on
 original x-y space is mapped to x'-y' space generated by SVD. x & y are clearly correlated. y' direction is insignificant & may represent some “noise", so we can remove it x' y' dj
Giả thiết của LSI
7.2 Query & Retrieval
Ak = UkΣkVkT
Query q in original space: q = UkΣkqkT
Since Uk are unit orthogonal vectors, UkTUk = I
UkTq = ΣkqkT
Σk-1UkTq = qkT
Finally, qk = Σk-1UkqT
Compare qk to documents in k-dimension space using similarity function
7.3 Example c1: c2: c3: c4: c5: m1: m2: m3: m4: human-computer interaction graphs
A=
Human machine interface for Lab ABC computer applications
A survey of user opinion of computer system response time
 EPS user interface management system
System & human system engineering testing of EPS
Relation of user-perceived response time to error measurement
 generation of random, binary, unordered trees
 intersection graph of paths in trees
Graph minors IV: Widths of trees & well-quasi-ordering
Graph minors: A survey c1 c2 c3 c4 c5 m1 m2 m3 m4 human interface computer user system response time
EPS survey trees graph minors
U=
V=
Ak =
Uk
Σk
Vk
Query “user interface" qk =
= (0.179-0.004) cosine(q,dj): c1: 0.964 c2: 0.957 c3: 0.968 c4: 0.928 c5: 0.922 m1: 0.022 m2: 0.023 m3: 0.010 m4: 0.127
Ranking: (c3,c1,c2,c4,c5,m4,m3,m2,m1)
7.3 Discussion
Pros: LSI give better results than traditional IR
- Cons:
Complexity of O(m2n), not suitable to Web search
Concept space is not interpretable as its description consists of all numbers with little semantic meaning. value of k needs to be determined based on specific document collection via trial & error, which is a very time consuming
Association rules may be able to approximate results of LSI
8. Web Search
Search Engine (Google, Bing, Baidu, Yandex)
Process: crawl pages, parse, index, store, query, retrieve
Parsing: Read HTML using lexical analyzer generator such as YACC & Flex
Indexing: small inverted index may be constructed based on terms appeared in title & summary. Full index built on entire page.
Searching & Ranking:
Using only relevance score of document may return low quality results.
Number of pages are very large, best results need to be on top
Hyperlink can be used to assess quality of each page to some extent. A link from page x to page y is an implicit conveyance of authority of page x to page y. That is, author of page x believes that page y contains quality or authoritative information. It makes use of link structure of
Web pages to compute a quality or reputation score for each page
Content factors:
Occurrence type: title, hyperlink, URL, body, tag
Count: number of occurrence of a term of each type
Position: Query terms that are near to each other are better than those that are far apart. Furthermore, query terms appearing in page in same sequence as they are in query are also better.
9. Meta-Search & Combining Multiple
Rankings
Meta-search engine combine results of multiple search engine increases search coverage of Web improve search effectiveness
Remove duplicate results (compare
URL, title)
Combine ranked results from individual search engines to produce a single ranking
Search
Engine 1
Search Interface
Metasearch
Engine
Search
Engine 2
Search
Engine k
9.1 Combine using Similarity Score
Set of returned document D = {d1,d2,…,dN}, search engine i return sij is similarity score of query to document dj
CombMIN(dj) = min(s1j,s2j,…,skj)
CombMAX(dj) = max(s1j,s2j,…,skj)
CombSUM(dj) = Σi=1ksij
CombANZ(dj) = CombSUM(dj) rj where rj number of non-zero similarity score
CombMNZ(dj) = CombSUM(dj) x rj
9.2 Combination using Rank Positions
Borda Ranking: For each search engine, first document has n point (n is number of documents), second document has n-1 point,…, rest point is divided evenly to all document that are not returned. Fina point of a document sum of points from all search engine
Condorcet Ranking: document A is ranked higher than document B if A is return by morn search engine than B. In same result from a search engine, ranked documents win unranked documents. All ranked document tie with 1 another. Final result is document(s) that wins pair-wise comparison.
Reciprocal Ranking: For each search engine, top document got 1 point, second got ½ point, third got 1/3 point, …, unranked ones got zero. Final point is sum of all points for all search engines
Borda:
Score(a) = 4 + 3 + 2 + 1 + 1.5 = 11.5
Score(b) = 3 + 4 + 3 + 3 + 3 = 16
Score(c) = 2 + 1 + 4 + 4 + 4 = 15
Score(d) = 1 + 2 + 1 + 2 + 1.5 = 7.5
Rank: b, c, a, d
Results:
Search engine 1:
Search engine 2:
Search engine 3:
Search engine 4:
Search engine 5:
Reciprocal Ranking:
Score(a) = 1 + 1/2 + 1/3 = 1.83
Score(b) = 1/2 + 1 + 1/2 + 1/2 + 1/2 = 3
Score(c) = 1/3 + 1/4 + 1 + 1 + 1 = 3.55
Score(d) = 1/4 + 1/3 + 1/4 + 1/3= 1.17
Rank: c, b, a, d.
Condorcet: a 1:4:0 2:3:0 3:1:1 b 4:1:0 2:3:0 5:0:0 c 3:2:0 3:2:0 4:1:0 d 1:3:1 0:5:0 1:4:0 - a,b,c,d b,a,d,c c,b,a,d c,b,d c,b
Win Lose Tie
Thứ hạng: c, b, a, d
10. Web spamming
Increasing content & influence on web will bring popularity & financial benefits to organizations & individuals
When users search for information through a search engine query, high-ranking websites benefit businesses, organizations & individuals that publish that website.
For a query, let's say web pages with more informational value rank higher. However, search engines do not "understand" information & make ratings based on syntactic & other surface features to evaluate information. Spammers take advantage of understanding search engine's ranking mechanism to build website content so that although it does not carry high informational value, it still ranks high
(SEO - Search Engine Optimization).
Web spam affects users making it difficult for them to find real information & reduces user experience; waste search engine crawling & storage resources & degrade search engine rankings
10.1 Content spamming
Content spamming builds website content (titles, meta tags, body, hyperlink phrases, URLs) related to certain queries
2 spamming technique:
Repeating some important terms: This method increases 
TF scores of repeated terms in a document & thus increases relevance of document to these terms. Add them randomly in an unrelated (or related) sentence
Dumping of many unrelated terms: This method is used to make page relevant to a large number of queries. Simply copy sentences from related pages on Web & glue them together. Add some frequently searched terms on Web & put them in target pages so that when users search for frequently search terms, target pages become relevant.
For example, to advertise cruise liners or cruise holiday packages, spammers put “Tom Cruise" in their advertising pages
10.2 Link spamming
Out-link spamming: add out-links in one's pages pointing to some authoritative pages to boost hub cores of one's pages. A page is a hub page if it points to many authoritative pages.
(e.g. Yahoo!)
In-link spamming:
1. Create honey pot: create a set of pages that contains useful information.
 honey pots attract people pointing to them because of useful information & then have high reputation scores. Honey pots contain
(hidden) links to target spam pages that spammers want to promote.
2. Adding links to Web directories
3. Posting links to user-generated content (reviews, forum discussions, blogs, etc)
4. Participating in link exchange: In this case, many spammers form a group & set up a link exchange scheme so that their sites point to each other in order to promote pages of all sites.
5. Creating own spam farm: In this case, spammer needs to control a large number of sites. Then, any link structure can be created to boost ranking of target spam pages.
10.3 Hiding technique
Hide spamming sentences, terms & links from user
Content hiding: Spam items are made invisible. Make spam terms same color as background color.
<body background = white>
<font color = white> spam items</font>
<a href = target.html"><img src="blank.gif"> </a>
Cloaking: Spam Web servers return a HTML document to user & a different document to a Web crawler. Identify Web crawlers in 1 of 2 ways
Maintains a list of IP addresses of search engines, identify crawlers by matching IP address
Use User-agent field in HTTP request mesage
User–Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)
Redirection: redirecting browser to another URL as soon as page is loaded. spammed page is given to search engine for indexing.
10.4 Combating Spam
Identify crawler as a regular Web browser
Give terms in such anchor texts higher weights
Link analysis method: PageRank, authority/hub point
Classification method. some example features used in detecting content spam
Number of words in page: A spam page tends to contain more words than a non-spam page so as to cover a large number of popular words.
Average word length: mean word length for English prose is about 5 letters. Average word length of synthetic content is often different.
Number of words in page title: Since search engines usually give extra weights to terms appearing in page titles, spammers often put many keywords in titles of spam pages.
Fraction of visible content: Spam pages often hide spam terms by making them invisible to user.
Partition each Web page into different blocks. Each block is given an importance level automatically. spam links are usually placed in unimportant blocks of page. links in less important blocks are given lower transition probabilities to be used in PageRank computation
Lesson 5:
Social Network Analysis
Big problems in Social Network Analysis
Graph Ranking: Analyze role of nodes in graph
◼ Community detection: Detect communities consisting of members of similar nature
◼ Link prediction: Predicting evolution of a graph over time
◼ Graph classification: Classify vertices & edges of graph into given classes
Agenda
1. Graph Ranking
2. Community Detection
3. Graph Representation
1. Graph Ranking
1.1 Basic concepts of graphs a) Undirected graph
Directed graph
Adjacent Matrix a[i, j]
1 if there is edge (i,j)
2 if there is a edge from a node to itself
0 otherwise
Degree of a node di(i) = number of in-edge of node i
◼ d (i) = number of out-edge of node i
1.2 Dijkstra algorithm
Find shortest path from source node s to other nodes of graph
◼ d(v): Distance from node s to node v
S1: Initialize d(s) = 0; d(v) = ꝏ
S2: Arrange nodes in a specific order in a queue Q
S3: Get node u from queue Q then update distance d(v) (if needed) of every node v adjacent to node u
Go back to step S2 until every node is computed
Example
Example (cont.)
1.3 Degree Centrality
Closeness Centrality d(i, j): shortest distance from node i node j
Betweeness Centrality pjk(i): Number of shortest path from node j to node k pass node i i
CB(1) = 15, CB(2) = CB(3) = CB(4) = CB(5) = CB(6) = CB(7) = 0
1.4 Prestige
Degree Prestige di(i): in-degree of node i
Proximity Degree
Ii: Set of nodes that can reach node i
1.5 PageRank Algorithm
Rank graphs based on general structure
◼ For large graphs, rank is approximated by an iterative algorithm based on 'random walk'
◼ Important applications in web search engines
◼ Cons: Doesn't depend on query
Hyperlink graph
Hyperlink graph (cont.)
Standardize:
Formula
R(A) = (1 – d) / N + d * ΣB:(B,A) ∈ E R(B) / do(B)
R(A): Thứ hạng của đỉnh A d: damping factor
N: số đỉnh của đồ thị
(B,A) cạnh của đồ thị do(B) bậc ra của đỉnh B
Example (d = 1)
Example (d = 0.85)
Algorithm
Algorithm PageRank(d, E)
1. Init page ranks R(0);
2. i = 1;
3. repeat
4. for each page A do
R(i)(A) = (1 – d) / N + d * ΣB:(B,A) ∈ E R(i-1)(B) / do(B);
6. endfor
7. i++;
8. until converged
Convergence speed
Application: Web Search
Application: Citation analysis
Guan et al. 2008. “Bringing Page-Rank to Citation Analysis"
Application: Citation analysis (cont.)
1.6 HITS Algorithm
Hypertext Induced Topic Search
J. Kleinberg. “Authoritative Sources in a Hyperlinked Environment." In
Proc. of 9th ACM SIAM Symposium on Discrete Algorithms (SODA'98), pp. 668–677, 1998.
Spam filtering
Query relevance
Execution
HIST
Online
PageRank
Offline
Authority/Hub
Authority: pages with many in-links
Hub: pages with many out-links
Bigraph
- Graph divided into 2 separated set of node such that every edge connects 2 node of different s
Algorithm
Input: Query q
Output: authority score & hub score of relevant pages of query q
Algorithm:
1 – Retrieve information
2 – Expand graph
3 - Compute rank
1-Retrieve information
Requires a search engine has relevant documents of query q
◼ Input query q & a root set W of top k pages relevant to
2- Expand graph
From root set W, expand to base set S
◼ For each page p in W
Insert pages that p links to
Insert pages that links to p
3- Compute rank
Authority score (a)
Hub score (h)
G = (V,E)
3- Compute rank (cont.)
Lesson 5:
Social Network
Analysis
2. Community detection
2.1 Community detection
Detect community in network
■ Community members are similar in nature
■ Communities can be related
■ number of communities depends on algorithm from Fortunato (2015)
a) Zachary's karate club b) Collaboration network between scientists working at Santa Fe Institute c) Lusseau's network of bottlenose dolphins from Fortunato (2015)
Community structure in protein–protein interaction networks from Fortunato (2015)
Overlapping communities in a network of word association from Fortunato (2015)
Community structure of a social network of mobile phone communication in Belgium from Fortunato (2015)
Network of friendships between students at Caltech from Fortunato (2015)
Map of science derived from a clustering analysis of a citation network from Fortunato (2015)
2.2 Kernighan–Lin algorithm
Minimum cut problem: Divide domain of undirected graph into 2 regions with same number of vertices so that sum of weights of edges connecting 2 clusters is minimal.
Algorithm
G = (V, E)
Separate nodes into to set A & B without overlap a ∈ A:
Intra-cost Ia = Σ u ∈ A ca,u
Inter-cost Ea = Σ ∈ B ca,v
Da = Ea – Ia b ∈ B, cost decrease if swap a và b
Told - Tnew = Da + Db – 2ca,b
Repeat find feasible pair (a,b) to reduce cost while sum of cost (of cut) decrease
Update cost
Example
Algorithm
Complexity
Initialize D: O(n2) (line 4)
■ Loop: O(n) (line 5)
■ Loop body: O(n )
Step i need (n – i + 1)2 time
Each loop: O(n3) (line 4-11)
■ Assume that algorithm terminate after r loops
■ Total time: O(rn )
Example
Example (cont.)
2.3 Girvan-Newman algorithm
Find bridges between communities based on “edge betweenness"
■ Repeat with each community to fin subcommunity
■ Final results are hierarchical tree with root is whole graph & leaves are nodes
Bridge & edge betweeness
Bridge: connect communities
■ Edge betweeness of an edge is number of shortest paths between pairs of nodes that run along it. If there is k shortest path between a pair of nodes, each path is 1/k
■ E.g: from node 1 to node 5 there 2 path, each has ½ flow unit
Bridge & edge betweeness
E.g: edge betweeness
Algorithm
ALgorithm:
1) Compute betweeness of every edges
2) Remove edges with highest betweeness
3) Recompute betweeness
4) Go back to step 2, repeat until there no edge left
Compute betweeness
Compute throughput using BFS
For each node u
1) BFS from node u
2) Find number of shortest path from u to other nodes
3) Find number of shoertest path from u to all nodes in graph
Comput betweeness
BFS for every node
■ Compute betweeness
■ Devide by 2 (each shortest path is counted twice)
Example
Step 1:
Example (cont.)
Step 2:
Example (cont.)
Step 3:
3. Graph Representation
Adjacent matrix is sparse, high dimension
■ Need representation of graph with low dimension
■ Application in graph analysis
node2vec
Input layer
Hidden layer
Output layer neighborhood(vi
SKIP-GRAM MODEL
Input layer
1-hot encoding for nodes
1 for current node, 0 for other
V dimension, V is number of nodes
Hidden layer
K dimension
■ Number of connection between input layer & hidden layer V x K
■ Connection weight between input layer & hidden layer is used as representation for nodes
Output layer
V dimension – number of nodes
■ Skip-gram model use current nodes to predict adjacent nodes neighborhood(vi)
■ Softmax activation function log-likelihood loss function
Neighborhood(vi)
BFS:
Sample using adjacent nodes of vi
Nodes in same community have same representation
DFS:
Sample using nodes in DFS order
Nodes in same roles have same representation (leaves, central, bridge)
Random walk: Balance between BFS & DFS
■ Sample size k (k = 3)
from Aditya Grover & Jure Leskovec. “node2vec: Scalable Feature Learning for Networks". KDD2016
Lession 6: OPINION
MINING
Content
1. Problems in opinion mining
2. Unsupervised Sentiment Analysis
3. Supervised Sentiment Analysis
1. Problems in opinion mining from M.D. Munezero et al. Are they different? Affect, feeling, emotion, sentiment & opinion detection in text, IEEE Trans.
Affect. Comput. 5 (2) (2014) 101–111
Applications
Customer service
■ Advertising, marketing
■ Social credit, personal finance
■ National security
■ Social policies
Problem 1: Sentiment Analysis
Classify comments & reviews into 1 of 3 classes:
Positive
Negative
Neutral
“BPhone 3, chất đến từng chi tiết"
Problem 2: Opinion summarization
Includes 2 sub-problems:
■ Define aspect
■ Categorize sentiment with each aspect
Problem 3: Comparative opinions
Comparative opinions
Object A & object B
■ Object A & object B on aspect s
■ Object A with other objects
Problems 4: Opinions Searching
Searching for Opinions on an object
■ search host architecture based
Problem 5: Opinions Filtering
Hype spam
Defaming spam
Good product
Bad product
Average product
2. Unsupervised Sentiment Analysis
2.1 Sentiment Analysis
Example
Sentiment
Introverted sentiment
It is an honor & pride for me to watch Vietnamese football playing at World Cup
Positive
Extroverted sentiment
Nur Farahain is also known as a friendly & sociable teacher with students.
Positive
Mood
 contestant was nervous, collapsed on table because of fatigue
Negative
Attitude
I wholeheartedly for my husband's family, but I'm still hated by my mother-in-law
Negative
Character
I consider myself quite active & know piano.
Positive
Problem definition
Requires sentiment recognition of a subject towards object mentioned in document.
- Simplify problem given subject & object assumption
Document
Sentiment
Logitech has a good battery, bought a B175 but battery with mouse has not been replaced for 3 years! Anyone who criticizes it, but I find Logitech mouse liked!
Positive
 expensive bad product even fake iPhone with a speaker underneath.
Negative
I'm using Logitech G502 & saw this 1......
Neutral
Methods of sentiment analysis
Methods knowledge base request
Custom request by field
Training data request
Sentiment dictionary
Unsupervised
Supervised
Sentiment Analysis based on dictionary thực_sự là mình rất sợ trà_sữa trân_châu . hầu_hết các cửa_hàng toàn nhập nguyên_liệu từ trung_quốc với giá rất rẻ , vì mình có thằng bạn nó cũng làm quán trà_sữa nó toàn lấy từ trung_quốc . thế mới có lãi cao vì thuê mặt_bằng rất đắt_đỏ rồi . nên các bạn hãy cân_nhắc có nên dùng trà_sữa ko nhé pos = 2 neg = 3 score = pos - neg = 2 – 3 = -1 < 0
Negative https://github.com/stopwords/vietnamesestopwords
Sentiment lexicon sợ negative rẻ positive lãi positive đắt đỏ negative cân nhắc negative
Supervised Sentiement Analysis unlabeled text labeled text
Evaluate labeling original text
Positive score
Knowled ge preproce ssing
Modeling
Predict
Negative score
2.2 Unsupervised Sentiment Analysis
P. Turney. “Thumbs Up or Thumbs Down? Semantic
Orientation Applied to Unsupervised Classification of Reviews". ACL'02
■ Algorithm:
B1. Extract opinion phrases
■ B2. Identify semantic/opinion orientation
■ B3. Determine sentiment
Apply to Vietnamese data
Step1. Extract opinion phrases
Identify language patterns with potential for opinion expression:
NN+JJ: commonnoun + adjective ('máy mới')
■ RB+JJ: adverb + adjective ('rất tốt')
■ RB+VA: adverb + verb adjective ('rất khỏe')
■ RB+VB: adverb + verb ('rất muốn')
■ VB+RB: verb + adverb ('chạy mượt')
Require document to be POS tagged
Step1. Extract opinion phrases (cont.) first word từ thứ hai
NN
JJ
RB
JJ/VA
RB
VB
RB
Thực_sự là mình rất sợ trà_sữa trân_châu . Hầu_hết các cửa_hàng toàn nhập nguyên_liệu từ trung_quốc với giá rất rẻ , vì mình có thằng bạn nó cũng làm quán trà_sữa nó toàn lấy từ trung_quốc . Thế mới có lãi cao vì thuê mặt_bằng rất đắt_đỏ rồi . Nên các bạn hãy cân_nhắc có nên dùng trà_sữa ko nhé
Step1. Extract opinion phrases (cont.)
First word
Từ thứ hai
NN
JJ
RB
JJ/VA
RB
VB
RB
Thực_sự là mình rất/RB sợ/VB trà_sữa trân_châu . Hầu_hết các cửa_hàng toàn nhập nguyên_liệu từ trung_quốc với giá rất/RB rẻ/VA , vì mình có thằng bạn nó cũng làm quán trà_sữa nó toàn lấy từ trung_quốc . Thế mới có lãi/NN cao/JJ vì thuê mặt_bằng rất/RB đắt_đỏ/VA rồi . Nên các bạn hãy cân_nhắc có nên dùng trà_sữa ko nhé
Step2. Identify Opinion Orientation
For each extracted phrase t, necessary to determine opinion orientation of this phrase, SO(t)
■ Assumption:
'tốt' : positive
■ 'kém': negative
SO(t) = sim(t, 'tốt') - sim(t, 'kém')
'tốt'
'kém
Step2. Identify Opinion Orientation
Determine similarity of 2 phrases based on likelihood of co-occurrence on a large corpus
Large Text Set: Web Text
■ Possibility co-occurrence: Pointwise Mutual Information
(PMI)
■ SO(t) = PMI(t; 'tốt') - PMI(t; 'kém')
Step2. Identify Opinion Orientation
PMI(t1; t2) =
Pr(t1, t2)
Pr(t1)Pr(t2)
Pr(t1| t2)
Pr(t2| t1)
Pr(t1)
Pr(t2)
P(t1): Probability occurrence t1 in corpus
P(t1| t2): Probability occurrence t1 when t2 2occurrenced
P(t1| t2) = (count(t1, t2) + 1) / (count(t2) + V)
P(t1) = (count(t1) + 1) / (sumt' count(t') + V)
V: Vocab size
B3. Determine sentiment
Assume document d consists a set of opinion phrases T extracted from step 2
For each t ∈ T, calculate SO(t)
Opinion orientation
SO(d) = sumt∈TSO(t)
SO(d) > 0: Positive document
SO(d) < 0: Negative document
3. Supervised Sentiment Analysis
Yoon Kim. “Convolutional Neural Networks for
Sentence Classification". EMNLP 2014
■ Using CNN model to classify reviews
■ Using pre-trained word embedding on a large data set as a word representation vector
■ Concatenate ordered word representation in text to serve as a 2D input signal for CNN
Word2vec
Using a neural network to learn a language model task:
CBOW: Use surrounding words in a window to predict center word
■ Skip-gram: Use focus words to predict surrounding words
Leverage large amounts of learning data without labeling (!)
■ Generates a vector representation of a word that exhibits some semantic relations.
CBOW
Hà_Nộ thủ_đ củ
Việt_Na
CBOW (cont.)
 input layer consists of V neurons that represent words in context in form: 1-hot
■ hidden layer consists of n neurons
■ output layer consists of V neurons used to predict center word
■ weight between input layer & hidden layer after learning is used as a lookup table of word representation
king – queen = man - ...
Vector representation 'king': a
■ Vector representation 'queen': b
■ Vector representation 'man': x
■ Calculate vector d = a – b + c
■ search word d' whose distance (Euclidean, cosine) to d is closest: d' ~ 'woman'
Visualization word representation
Skip-gram
Hà_Nội là thủ_đô của
Việt_Nam
Model architecture
Input layer xi ∈ Rk is continuous representation of word i
Randomly initialized & weights updated during learning
■ Initialized based on a pre-trained representation of a large corpus
Updated during training
“Freeze" in training
Input consists words x1, x2,…, xn in order
■ Document representation is a concatenation of word representations in order appear in document
Convolutional layer
Each filter w ∈ Rhk scans a window of h consecutive words xi:i+h to generate a feature ci
Window width: h
■ Window height = word embedding dimension
Each filter generates a feature map c ∈ Rn-h+1, c = [c1, c2, …, cn-h+1]
Pooling layer
For each feature map c, apply max pooling to get maximum value
■ Apply windows h ∈ [3, 4, 5]
■ For each value of h there are 100 filters
■ Total number neurons in pooling layer: 100 x 3 =
Fully connected layer
Adjustment technique: Apply dropout at pooling layer with dropout ratio p = 0.5
■ Number neurons in output layer:
2: only positive & negative labels
■ 3: positive, neutral, negative
Dataset
MR: Movie commentary with each comment being a sentence. Label positive/negative
■ SST-1: Extension MR set with 5 labels (very positive, positive, neutral, negative & very negative)
■ SST-2: Similar to SST-1 but removes neutral label & has only 2 positive & negative labels
■ CR: Product reviews. Label positive/negative.
Models
CNN-rand: Embedded words are randomly initialized & updated during training
■ CNN-static: Using pretrained word2vec embedding, word representations (including randomly initialized
OOV words) are kept weighted
■ CNN-non-static: initial word representation in word2vec is fine-tuned during training
■ CNN-multichannel: hybrid static & non-static
Experimental results
Word embedding fine tuning
Lesson 6:
Opinion Mining
Component Problems
1. Aspect extraction
“The voice quality of this phone is amazing"
“I love this phone" (GENERAL aspect)
2. Aspect-based Opinion Mining
“The voice quality of this phone is amazing" → Positive
“I love this phone" → Positive
Aspect-based Opinion Mining
Supervised approach
Use dependency syntax for extracting syntactic features
High results but difficult to adjust to new fields
Classical approach
High results on multiple fields
Requires knowledge in language & specific field,
Contains many rules
Agenda
[1] Sentiment Ontology Tree
[2] Sentiment Analysis on Twitter
[3] Sentiment Analysis on social network
[4] Entity detection & assignment
[5] Comparison Mining
[1] Sentiment Ontology Tree
Hierarchical classification based on hierarchical learning technique
■ Sentiment Ontology Tree (SOT):
Demonstrate relationships ancestors - descendants among aspects in domain
Each aspect comes with nodes that express sentiment for that aspect
Example of SOT
SOT
T(v, v+, v−, T)
■ v: root node express property v
■ v : positive node of property v v-: negative node of property v
■ T: set of sub-SOT of T: T′(v′,v′ ,v′ ,T′)
HL-SOT sentence x ∈ X, X = Rd
■ Set of nodes in tree: Y = {1, 2, …, N}
■ Label vector of x: y = {y1, y2, …, yN} ∈ {0,1}
∀i ∈ Y, yi = 1 if x is labeled by classifier of node i yi = 0 if x is node labeled by classifier of node i
Problem definition y ∈ {0,1}N adapt a SOT if & only if
∀i ∈ Y, ∀j ∈ A(i): if yi = 1 then yj = 1, where A(i) is set of ancestor node of node i
Let set of label vectors adapt SOT is τ
■ Hierarchical classifier f: X → τ generate vector y for each input vector x so that y satisfied a SOT
HL-SOT y = f(x) = g(W · x)
■ W = (w1, ..., wN) wi is weight of linear classifier of node i yi = wiTx ≥ θi if i is root or yj = 1 where ∀j ∈ A(i); otherwise yi = 0 θi threshold of classifier of node i
Learning weight
Training dataset D = {(r, l) | r ∈ X, l ∈ Y}
■ Weight matrix W is initialized = 0
■ Threshold vector θ is initialized = 0
Learning weight (2)
For each example rt, weight is updated as follow:
Idxd : identity matrix
■ Q(i, t-1): number of times parent node of node i have been set positive before
■ Si,Q(i,t-1) = [ri,1, ..., ri,Q(i,t-1)]
■ Only update weight wi,t of node i if parent node of i is set positive
Learning weight (3)
Update threshold of classifier where ε small real number to control update speed
If classifier predict correctly, keep as it is
■ If classifier predict incorrectly, as positive, increase θ
■ If classifier predict incorrectly, as negative, decrease θ
Learning algorithm
[2] Sentiment analysis on Twitter
Tweet has maximum 140 characters
■ 2011: Twitter has 190M users, 65M tweet per day
■ User express sentiment on Twitter
Sentiment analytic tools on Twitter: Tweetfeel, Twendz,
Twitter Sentiment
Characteristic of tweet
Tweet is shorter & more ambiguous then product review
■ Product review has known target object; while tweet need an extra step to determine target object
■ Relevant tweet provide context information for classifier
■ General classifier (for general objects) is not appropriate for tweet classification
Problem definition
Input: set of tweets consist target objects
■ Output: analyze sentiment of tweets for target objects
Neutral:
Positive
Negative
Algorithm
1. Subjective/Objective classification: If tweet is objective
→ Neutral
2. Positive/Negative classification
3. Optimize graph of relevant tweets
Classify using linear SVM
Preprocess
Tag part-of-speech using OpenNLP
■ Stemming using 20,000 words dictionary (e.g. 'playing'
→ 'play')
■ Normalize using simple rules (e.g 'gooood' → 'good',
'luve' → 'love')
■ Analyze dependency syntax using Minimum Spanning
Tree
Independent features
Content features: words, punctuations, emoticon, hashtag
■ Vocabulary features: sentiment vocabulary of General
Inquirer
■ These are typical features used in general sentiment classifier
Extra objects
1) Noun phrases
“I am passionate about Microsoft technologies, especially
Silverlight"
2) Extend using co-occurrence resolution
“Oh, Jon Stewart. How I love you so."
3) Top K nouns & noun phrase are related to original object using PMI
Extra objects (2) p(w,t): probability that w & t appears in same corpus p(w): probability w appears in corpus
■ p(t): probability t appears in corpus
■ K = 20, corpus consists of 20M tweet
Extra objects (3)
4) Central words of noun phrase if PMI greater than a threshold
“Microsoft technologies" → 'technologies'
“the price of iPhone" → 'price'
“LoveGame by Lady Gaga" → 'LoveGame'
Features depend on object
Object T
■ wi_arg2: transitive verb accepts T as object
“I love iPhone" → 'love_arg2' wi_arg1: transitive verb accepts T as subject
■ wi_it_arg2: intransitive verb accepts T as subject
■ wi_arg1: noun or adjective accepts T as central words
(in noun phrase)
Features depend on object (2) wi_cp_arg1: noun or adjective links to T using a copula
(e.g. “to be")
■ wi_arg: adjective or intransitive verb appears as a imdepent senetence & T appears in previous sentence
“John did that. Great!" → 'great_arg' arg1_v_wi: adverb support modifies verb that accepts T as subject
“iPhone works better with CellBand" → 'arg1_v_better'
Features depend on object (3)
If feature is modified by a negative word, add prefix "neg-"
“iPhone does not work better with CellBand" → 'arg1_v_negwell', 'neg-work_it_arg1'
Set of negative words: not, no, never, n't, neither, seldom, hardly
Graph optimization
Using only content to classify sentiments can be incorrect for short tweet
■ Requires more context information:
Retweet: keep original content
User's tweets have information about object in a short window of time: assume that user do not change sentiment about object
Reply: answer a tweet or feedback
Graph of tweet
Solid line: tweet of same user
■ Dash line: retweet
■ Dot line: reply
Model evaluation
Query set {Obama, Google, iPad, Lakers, Lady Gaga}
■ Data: 459 positive, 268 negative & 1,212 neutral
■ Consensus degree: 86% of 100 tweet
1 positive-negative tweet
13 neutral-negative/positive tweet
Feature evaluation
[3] Sentiment analysis on MXH
Analyze sentiment of product review
■ Multilingual: English, French, Dutch
■ Label set = {positive, negative, neutral}
Use linguistic features
■ Features based classifier
Vocabulary features
Unigram: word/token in sentence; remove stopwords
(publist.com)
■ Stem: using Porter algorithm (e.g. 'playing' → 'play')
■ Negative: vd “not worth"
Document features: e.g. “même si le film a eu beaucoup de succès, je le trouvais vraiment nul!" (even though movie had a lot of success, I really found it nothing!)
Syntax features
Dept difference: between word/entity features in revert syntax tree & weight of word (English, Dutch)
■ Distance : distance (by BFS) between word/entity features in revert syntax tree & weight of word
(French)
Example of syntax feature
Depth('giá') = 1
■ Depth('Iphone') = 2
■ Path_distance('giá', 'Iphone') = 1 http://45.117.171.213/bknlptool/
Difference in languages
Compound noun: 'topfilm' (top movie)
■ Composed verb: “tegenvallen, valt tegen" (to be below expectations), “meevallen, valt mee" (turn out better than expected)
■ “je ne suis pas d'accord" (I don't agree)
■ “L é tro bel cet voitur" (Elle est trop belle cette voiture She is too beautiful, this car)
■ j'aime ce film (I love this movie) /ce film est bien (this movie is good)
■ de film is goed (this movie is good)
Classifiers
Support Vectors Machines (SVMs): classifier learns support vectors to classify object into 2 classes so that margin is maximum (Weka)
■ Multinomial Naive Bayes (MNB): Multiclass classifier based on Bayesian probability with assumption of probabilistic independence between features(Weka)
■ Maximum Entropy (ME): classifier is based on maximum of entropy (MaxEnt)
Cascade model
Active learning
Target: Good quality model learning with minimal amount of labeled data
■ Method: Automatic selection of examples to label from an unlabeled set (possibly based on some original labeled examples)
Uncertain sampling
Criteria: Select examples where classifier already has uncertainty. Degree of uncertainty based on :
Probabilistic Classifiers(MNB, ME)
Distance to hyperplane (SVM)
Target:
Reduce redundancy in training data
Improved ambiguous examples (various emotions, different feelings for different entities)
Dataset
User product comments & news articles, product reviews on forums, news sites
■ Blog: skyrock.com, livejournal.com, xanga.com, blogspot.com
■ Review pages: amazon.fr, ciao.fr, kieskeurig.nl
■ Forums: fok.nl, forums.automotive.com
■ Outliers: advertisement, spam, personal style
Dataset (2)
Target objects: cars, movies; replace name with generic label 'CAR' or 'MOVIE'
■ Remove questions
■ Sentence level classification consensus: kappa = 82%
■ Each language has 2500 neutral examples, 750 positive examples & 750 negative examples
■ Evaluation using 10-fold cross-validation
Experiment settings
Cascade model:
Level 1: unigram
Level 2: + discourse + negation
Level 3: + parsed feature
SC uni-lang (~ level 2) train on all data
SC uni-lang-dist add features of distance between words & entities
English: MNB; Ducht: SVM; French: ME
Impact of features
Impact of bagging
Impact of cascade model
Efficiency on neutral example
Impact of domain
Impact of syntax features on ambiguous examples
Impact on uncertain examples
Impact on outliers
Error analysis
Error analysis (2)
1. Lack of training data results in sentences that don't carry emotions but are labeled as positive/negative because they contain words that often appear in positive/negative sentences
2. Ambiguous sentences
3. Sentences with feelings towards another entity
4. Sentences with negative words
5. Emotions are expressed through metaphors
6. Emotions must be inferred from broad context (text) or general knowledge
Error analysis (3)
7. Requires knowledge of a narrow field
8. Emotions expressed through idioms
9. Emotions expressed through homonyms
10. Language-specific problems such as compound words in Ductch or accents in French
Error analysis (4)
5. right
6. felt
7. mieux
A Good Year is a fine example of a top-notch director & actor out of their elements in a sappy romantic comedy lacking in … certainly more comfortable & rewarding than an Audi Q7 & … it is een schot in de roos (this is a shot in bull's eye) ~ they got it exactly
I don t know maybe it s because I was younger back then but Casino Royale more like a connect dots exercise than a Bond movie.
[...] attention pour avoir une chance de ne pas dormir au bout de 10 minutes, vaut connaı̂tre les règles du poker […] (dormir ~ sleep)
Casino Royale finally hits full-throttle in its second hour but Bond fans will find movie hit-&-miss at best
Not a coincidence–GM used Mercedes&#39; supplier for new ... interior plastics & wood trims is REALLY cheap. ... brown seats in a light colored car only make
A Ferrari is not cheap to buy or run & residual values weaken if you use car regularly.
Evaluate active learning
Sample until have 500 example
■ Using MNB classifier with unigram + discourse + negation features
■ Evaluation on English
Validation set: 1703 example: 1151 neutral, 274 positive & 72 negative
Evaluate active learning (2)
Evaluate active learning (3)
Unvertain sampling & stochastic sampling
Thank you for your attentions
Lession 5: OPION
MINING (cont.)
[4] Entity detection & assignment
In product reviews, it is often known who is being reviewed
■ However, on forums, it is necessary to identify entity audience comments are aimed at
Task 1: Detect entity in sentence
■ Task 2: Assign entity to sentence (do not specify entity to be evaluated)
Assumption of emotional homogeneity
Eg.1:
(1) I bought Camera-A yesterday. (2) I took some pictures in evening in my living room. (3) images are very clear. (4)
They are definitely better than those from my old Camera-B. (5)
 battery is very good too.
Eg.2:
(4) → Camera-A > Camera-B
(1) I bought Camera-A yesterday. (2) I took a few pictures in evening in my living room. (3) images were very clear. (4)
They were definitely better than those from my old Camera-B. (5)
 pictures of that camera were blurring for night shots, but for day shots it was ok
Problem statement
A thread t containsposts <p1, p2,…, pn>
■ A post p contains sentences <s , s ,…, s >
■ A sentences s contains an entity set reviews ε is a subset of set of all entities E = {e1, e2,…}
■ An entity e may be explicitly or implicitly in a sentence s
Problem statement (2)
Ex: “Camera-A looks really pretty. battery lasts very long"
■ Most sentences refer to only 1 entity (|ε|=1)
■ Sentences involving more than 1 entity are usually comparative sentences (|ε|=2)
“Camera-A is better than Camera-B"
Assuming sentences in a post are all meant to evaluate entity object (in fact, there are also unrelated sentences, e.g. greeting)
Problem statement (3)
Given a set T threats in domain:
Task 1 - Entity Detection: Detect set of entities E in T
■ Task 2 - Entity Assignment: Assign each sentence in T with 1 or several entities in E
Task 1 - Entity Detection
Unsupervised method based on sequential pattern mining using an original entity set E(0) = {e1, e2, …, en}
Step 1. Prepare data
Step 2. Sequential pattern mining
Step 3. Extract candidate
Step 4. Filter candidates
Step1. Prepare data
Search for all sentences containing entities in original set; replace entity name (containing 1 or more words) with generic name ENTITYXYZ
■ Generate series by selecting window 5 from before & after entity; each element is a word/word of type
Hiiiiiiiii/NNP SK/NNP -/: ,/, dont/NN be/VB mad/JJ everyone/NN doesnt/NN have/VBP a/DT n95/CD phone/NN fetish/NN ducky/JJ mad/JJ everyone/NN doesnt/NN have/VBP a/DT ENTITYXYZ /CD phone/NN fetish/NN ducky/JJ
<{JJ, mad}{NN, everyone}{NN, doesnt}{VBP, have}{DT, a}{CD, ENTITYXYZ}{NN, phone}{NN, fetish} {JJ, ducky}>
Step 2. Sequential pattern mining
Min support = 0.01
■ Patterns must contain {POS, ENTITYXYZ}
■ Patterns must have length >= 2
■ E.g.: <{IN}, {DT}, {NNP, ENTITYXYZ }, {is}>
Step 3. Extract candidate
Search for entities match generated patterns
/DT misses/VBZ has/VBZ currently/RB got/VBN a/DT Nokia/NNP 7390/CD at/IN /DT end/NN of/IN /DT day,/VBG all/DT she/PRP does/VBZ is/VBZ text/NN &/CCmake/VB calls,/NN but/CC /DT reception/NN is/VBZ terrible,/VBG where/WRB my/PRP$ 6233/CD would/MD get/VB full/JJ bars/NNS hers/PRP would/MD only/RB get/VB 1/CD or/CC 2./CD
<{DT}, {NNP, ENTITYXYZ}, {CD}> ~ a/DT Nokia/NNP 7390/CD
<{DT}, {NNP}, {CD, ENTITYXYZ}, {IN}> ~a/DT Nokia/NNP 7390/CD at/IN
Step 4. Filter candidates
Eliminate entities whose POS is different from 
POS most popular with this candidate
■ For example, 'accessories' is usually labeled NNS so
'accessories/CD' will be excluded
You/PRP can/MD also/RB be/VB sure/JJ it/PRP will/MD work/VB with/IN all/PDT /DT Sony/NNP Ericsson/NNP walkman/NN phone/NN accessories/CD
<{IN}{DT}{CD, ENTITYXYZ}> → accessories (sai)
Step 4. Filter candidates (2)
Use <Brand Model> (“Moto Razr V3") to search for brand & model pair
■ Use syntax patterns to find competing brands: A &
B; A or B; A vs B; A more than B
<Brand
<Model
As/RB far/RB as/IN I/PRP heard/VBD Nokia/NNP N95/CD seems/VBZ to/TO be/VB /DT leader/NN in/IN this/DT sense./CD
Task 2 - Entity assignment
Comparison sentence
Comparative: “Camera-X's battery life is longer than that of Camera-Y"
■ Equal: “Camera-X & Camera-Y are of same size"
■ Non-comparable: “Camera-X & Camera-Y have different shapes"
■ Superlatives: “Camera-X's battery life is longest"
Unify emotion
Suppose entity e first appears in sentence s0 & next sentence is s1 .
■ (1) If s0 is a normal sentence
If s1 is a normal sentence then it is assigned to e
■ If s1 is a comparison sentence, e will be compared with a new entity (needs to be introduced).
(2) If s0 is a comparative sentence
If s0 is a comparative sentence; s1 represents positive/negative emotion & contains no entity then it is assigned to better/worse entity
Unify emotion (2)
If s0 is an equal or non-comparable sentence, because we are not sure which entity s1 refers to, we assign it to entity that came before s0.
If s1 is a comparative, s1 is assigned to entity in s1
(3) If s0 is superlative sentence
If s1 is a normal sentence, we assign it to best entity mentioned in s0 .
■ If s1 is a comparative, s1 is assigned to entity in s1
Algorithm si.entity:
Entity mentioned in si si.superiorEntity: Better entity in comparative sentence si.inferiorEntity: worse entity in comparative sentence opinion(): Function to determine emotions in normal sentences compOpinion():
Function to determine emotions in comparison sentences
Sentiment Analysis
Analyze sentiment of a sentence towards an entity assigned to sentence based on evidence::
Opinion words: great, good, bad, poor; “the battery of this camera lasts long"/ “This program takes a long time to run"
■ Opinion phrases: “cost someone an arm & a leg", “a good deal of"
■ Negative: not, “not only ... but also"
■ Clause 'but': “The picture quality is great, but not battery life"
Specification language like[VB] => Po
Example
 picture quality of this camera is not good, reaction is too slow, but battery life is long.
 picture quality is not[Ng] good[Po], reaction is too slow[Neu], but[But] battery life is long[Neu]. too + Neu[JJ][T] => NE
 picture quality is not[Ng] good[Po], reaction is too slow[NE], but[But] battery life is long[Neu].
 picture quality is not[Ng] good[Negative], reaction is too slow[NE], but[But] battery life is long[Neu].
Analyze comparative sentences
Comparative sentence matches 1 of patterns: a). pronoun + compkey + prodname,
■ b). prodname + compkey + pronoun,
■ c). prodname + compkey + prodname
■ d). pronoun + superkey
■ e). prodname + superkey
■ f). as + JJ + as (ngoại trừ “as long as" và “as far as")
Where compkey is comparison word, prodname is product name, superkey is comparative word
Analyze comparative sentences (2)
Short adjectives/adjectives are changed to more/most by adding -er/-est (higher/highest)
■ Some irregular cases: good/better/best
■ Longer adjectives/adverbs add more/most
■ Apply rules: more/most + Pos → Positive more/most + Neg → Negative less/least + Pos → Negative less/least + Neg → Positive
Other words like win, prefer, superior, inferior
“In term of battery life, Camera-X is superior to Camera-Y"
Result evaluation
Dataset:
HowardForums: Movie reviews
■ AVSforums: Plasma/LCD TVs, Projectors & DVD players
Result evaluation (2)
CRF: Entity Detection by CRF
■ NET: Entity Detector
■ Baseline1: Get last entity of previous sentence if current sentence does not contain entity.
■ Baseline2: Gets first entity of previous sentence if current sentence does not contain an entity.
■ ED (k-com): Given comparative sentences
■ ED (unk-com): Need to detect comparative sentences
Result evaluation (3)
[5] Exploiting comparative sentences
User reviews on Internet for products:
90% din form of direct reviews (“the picture quality of
Camera X is great")
■ 10% as comparison (“the picture quality of Camera X is better than that of Camera Y")
Problem statement
Many comparative sentences don't have a direct comparison word, emotion of same word depends on context
■ “the battery life of Camera X is longer than Camera
Y"
■ “Program X's execution time is longer than Program
Y"
■ Choosing sentences as context leads to including a lot of irrelevant information
Problem statement (2)
Context: evaluated entity + comparison word
■ How to identify emotions expressed by context?
■ Using external knowledge (epinions.com) to identify opinion orientation from context
Epinions.com clearly separates positive & negative comments
■ what context often appears in positive or negative comments?
Problem statement (3)
Given a relation corresponding to comparative sentence
“Camera X has longer battery life than Camera Y"
<C (comparison word), F (feature), e1 (entity 1), e2 (entity
2), type (comparison type)>
<longer, battery life, Camera X, Camera Y, comparative>
Determine which entity is 'better'
Pros và Cons
Pros: great photos easy to use good manual many options takes videos
<photo>
<use>
<manual>
<option>
<video>
Type 1 -er/-est
Short adjectives/adverbs add -er/est
■ C express emotion (better/best): If positive, choose e1, if negative, choose e2
■ C no emotion, F express emotion
“Car X generates more noise than Car Y"
■ C comparative + F positive → e1
■ C comparative (reduction) + F positive → e2
■ C comparative + F negative→ e2
■ C comparative (reduction) + F negative → e1
Type 1 -er/-est (2)
Both C & F no sentiment
Nếu OSAP(F,C) > OSAN(F,C) → e1; otherwise chooose e2
Type 1 -er/-est (3)
Calculate OSAP(F,C):
Count number times C (& synonyms) & F (& synonyms) appear together in Pros
■ Count number times antonyms of C & F appear in
Cons
■ Count number of times C & antonyms of F appear together in Cons
Use Wordnet to get synonyms & antonyms
■ Do same with OSAN(F,C)
Type 1 -er/-est (4)
If C exhibits a feature
“Camera X is smaller than Camera Y"
■ Count number times C occurs in Pros & Cons & choose larger value
Type 2 more/less + adj/adv
Adjectives/adverbs express emotions
“Car X has more beautiful interior than Car Y"
Adjectives/adverbs don't express emotions
Adjectives/adverbs describe features
Adjectives/adverbs don't describe features
Negative:
“Camera X's battery life is not longer than that of Camera
Y"
Evaluation of results
 data includes camera ratings, DVD players, MP3 players, Intel vs AMD, Coke vs Pepsi, Microsoft vs
Google; laptop, mobile phone
Evaluation of results (2)
Baseline-84%: Always take first entity
Lesson 7: Information
Extraction
Content
1. Information extraction system architecture
2. Named Entity Recognition
3. Unsupervised relation extraction
4. Remote supervised
5. Coreference resolution
1. Information extraction system architecture
Information extraction is process of finding entities & relationships between these entities in a text
■ Extracting information for text mining is more precise & concise than tasks such as text classification or text labeling.
■ Predefined entity types & relationships
Assumptions of information extraction
Information is presented explicitly & requires no inference
■ A small number patterns can summarize content of text
■ Necessary information appears locally in text
Types of information extracted
Entities: People, organizations, places, etc.
■ Attributes (of entity): Title, age, type of organization...
■ Reality: relationship between employees & company, relationship between viruses & diseases, etc.
■ Events: 2 companies merging, earthquake, terrorism,...
Information extraction system architecture
Partitions
Word Tokenize
Pos-tag
Morphological & semantic analysis
Semantic parsing
Shallow parsing
Parsing
Deep parsing
Coreference resolution
Field analysis
Association
Named Entity Recognition
Detects named entities in text & classifies into predefined classes
[Forbes]ORG : [Việt Nam]LOC có 4 tỷ phú
Clustering
Detect noun & verb phrases in sentences
Trong đó , Việt Nam có 4 đại diện là Chủ tịch Vingroup Phạm Nhật
Vượng , CEO VietJet Air Nguyễn Thị Phương Thảo , Chủ tịch
Thaco Trần Bá Dương và Chủ tịch Techcombank Hồ Hùng Anh .
Relation Extraction
Extract relationships between entities (attributes, events)
Goldman Sachs Group thì đi vay tiền của Cục Dự trữ Liên bang Mỹ.
Aikido là một môn võ thuật Nhật Bản hiện đại
Coreference resolution
Detect occurrence of same entity as different references
Aikido1 là một môn võ thuật Nhật Bản hiện đại được phát triển bởi
Ueshiba Morihei2 như một sự tổng hợp các nghiên cứu võ học , triết học và tín ngưỡng tôn giáo của ông2 . Aikido1 thường được dịch là " con đường hợp thông ( với ) năng lượng cuộc sống " hoặc " con đường của tinh thần hài hòa " . Mục tiêu của Ueshiba2 là tạo ra một nghệ thuật1 mà các môn sinh3 có thể sử dụng để tự bảo vệ mình3 trong khi vẫn bảo vệ người tấn công4 khỏi bị thương . Các kĩ thuật của Aikido1 bao gồm : irimi ( nhập thân ) , chuyển động xoay hướng ( tenkan - chuyển hướng đà tấn công của đối phương4 ) , các loại động tác ném và khóa khớp khác nhau .
2. Named Entity Recognition
Based on dictionary:
Can detect common entities
■ Request to build a dictionary of own names
■ Can't handle ambiguity
Based on regular expression
Using expert knowledge
■ Common patterns can be detected
Based on machine learning
Request training data
■ Accuracy does not vary much between fields
■ Problem of labeling string BIO
Input is a sentence
■ output is label of each word in sentence
Label string BIO
B: Begin
■ I: Inside
■ O: Outside
BORG
IORG
O O O
O BORG
IORG
Goldman Sachs Group thì đi vay tiền của Cục Dự_trữ Liên_bang Mỹ
Feature set
Words in window [-k, k] (k = 2, 3)
■ Word form:
Uppercase, lowercase
■ Number
■ Punctuation
Word type: Output of word-type labeling problem
■ Word space: Output of clustering problem
NER based on CRF
[1]: Using PoS & standard clustering
■ [2, 3]: PoS & automatic clustering by NNVLP engine & Underthesea
■ [4]: No PoS & clustering from P. Q. N. Minh. “Feature-rich CRFs for Vietnamese NER". CICLING 2018
Evaluation
[1]: Using standard PoS
■ [2-6]: Automated PoS from tools
■ [7]: No PoS & clustering
Evaluation (cont.)
[1]: Use standard word separator
■ [2,3]: Automatic tokenizer using UETSegmenter &
RDRSegmenter
Evaluation (cont.)
[1]: syllable-based model (no word tokenize)
■ [2]: Use standard separator
[3]: Automatic word tokenizer with RDR Segmenter tool
Evaluation (cont)
Word: word in window
■ Word shapes: word form
■ w2v: word embedding
■ Cluster: Brown clustering representation
NER base on RNN from Nguyen et al. “Neural sequence labeling for Vietnamse POS tagging & NER". RIVF 2019
Input layer
Combined Embedded Representation:
Word representation: Using word embedding pre-trained by word2vec on 2 million documents
■ Character representation: Using bidirectional LSTM network to learn character representation with random initialization
■ Word performance: 1-hot representation
■ Cluster representation: 1-hot representation
Learn to represent characters
Bidirectional LSTM
Using 2 LSTM networks in forward & reverse direction
Purpose: Words at beginning of a sentence can use both information at end of sentence to make predictions & vice versa
Outputs are coupled to feed into output layer
Output layer
Predict BIO labels for entity types
For example: With 3 entity types ORG, PER, LOC, label set has 7 labels (B-ORG, I-ORG, B-PER, I-PER, BLOC, I-LOC, O)
 output layer can be fed into a model of CRFs to represent relationship with label at a previous point in time through transition probabilities.
Evaluation
BiLSTM-CRFs use additional PoS & clustering information
BiLSTM-CRFs don't incorporate character level representation
3. Unsupervised Relation Extraction
Supervised learning is highly accurate but requires training data
■ Unsupervised learning takes advantage of large amounts of data but has less accuracy
■ Remote supervised leverages knowledge base & improves accuracy over unsupervised learning
Snowball
Seed Tuples
Find Occurrences of Seed
Tuples
Entity Labeling
Generate New Seed Tuples
Augment Table
Generate Extraction
Patterns
Seed Tuples
User-provided
■ Then system automatically extracts from text
■ Ex: Relationship <tập đoàn, trụ sở>
<Microsoft, Redmond>
■ <Exxon, Irving>
■ <IBM, Armonk>
Search seed tuples
“Hệ thống máy chủ của Microsoft nằm ở trụ sở chính Redmon"
■ “Exxon, Irving đang dần trở thành tập đoàn dầu khí..."
■ “Tin đồn rút nhân viên khỏi Iraq đến từ trụ sở chính của Exxon, Irving..."
■ “… vừa nhận được email từ trụ sở chính của Boeing ở Seattle."
Entity Labeling
“Hệ thống máy chủ của <ORG> nằm ở trụ sở chính
<LOC>"
■ “<ORG>, <LOC> đang dần trở thành tập đoàn dầu khí..."
■ “Tin đồn rút nhân viên khỏi Iraq đến từ trụ sở chính của <ORG>, <LOC>..."
■ “… vừa nhận được email từ trụ sở chính của <ORG> ở <LOC>."
Generate 5-tuple
5-tuple: <left, tag 1, diddle, tag 2, right>
■ Left: k words to left along with weight vector
■ Tag 1: first entity
■ Middle: words in middle along with weight vector
■ Tag 2: second entity
■ Right: k words to right along with weight vector
Generate 5-tuple (tiếp)
{<trụ sở,
<chính, 0.3>}
Microsoft
{<nằm, 0.6>,
<ở, 0.9>}
Redmond
<left, tag 1, diddle, tag 2, right>
Generate 5-tuple (cont.)
{<trụ sở,
<chính, 0.3>}
{<nằm, 0.6>,
<ở, 0.9>}
ORG
LOC
{<đang, 0.2>, <dần, 0.1>,
<trở_thành, 0.15>}
{<trụ sở, 0.6>,
<chính, 0.3>, <của, 0.5>}
ORG
LOC
{<trụ sở, 0.6>,
<chính, 0.3>, <của, 0.5>}
ORG
{<ở, 0.95>}
LOC
Generate Extraction Patterns
Given 2 5-tuples with same tag1 & tag2: t = {l, tag1, m, tag2, r}
■ t' = {l', tag , m', tag , r'}
Similarity: match(t, t') = l·l'+ m·m' + r·r'
■ Clustering 5-tuples based on similarity
■ For each cluster, take centroid of c as extraction patterns p = {lc, tag1, mc, tag2, rc}
Generate New Seed Tuples
Algorithm GenerateTuples
1. foreach paragraph ∈ corpus do
{<o, l>, <ls, t1, ms, t2, rs>} = CreateOccurrence(paragraph);
TC = <o, l>;
SimBest = 0;
5. foreach p ∈ Patterns
6. sim = Match(<ls, t1, ms, t2, rs>, p);
7. if (sim ≥ τsim) then
UpdatePatternSelectivity(p, TC);
9. if (sim ≥ SimBest) then
SimBest = sim;
PBest = p;
12. endif
13. endif
14. endfor
15. if (SimBest ≥ τsim) then
CandidateTuples[TC].Patterns[PBest] = SimBest;
17. endif
18. endfor
19. return CandidateTuples;
Patterns Evaluation for each example <org, loc>, classify:
Positive if an pattern already exists
■ Negative if exists pattern <org, loc'>
■ Unknown if <org, *> not exist yet
Confidence of sample P:
P.positiv conf(P) =
P.positive + P.negative
P.positive: number positive examples matching P
■ P.negative: number negative examples matching P
Example Evaluation
Example confidence T = {org, loc}
P = {Pi} set of patterns that generate for example T
■ C is 5-tuple corresponding to text matches P with similarity Match(Ci, Pi)
■ Pattern example set= {T| Conf(T) > τ }
Pros, Cons
Advantages:
Take advantage of unlabeled data
■ Just a handful of original pattern examples
Defect:
Still requires manual labeling from users
■ Iterative process leads to quality degradation
4. Remote supervised
Freebase is a large & quality knowledge base about relationships between entities
■ Freebase is built from Wikipedia
■ Remote supervised:
Freebase supervises process of extracting relations from text
■ Freebase + corpus = labeled data
Remote supervised
{<entity1, relationi, entity2>}
Freebase corpus match
Sentence 1: …. entity1…. entity2….
Sentence 2: …. entity2…. entity1….
Sentence n: …. entity2…. entity1….
({features}, relationi)
{features}
{<entity1', relationi, entity2'>} → ({features'}, relationi)
{<entity1'', relationj, entity2''>} → ({features''}, relationj) multiclass classifier f: {relation1, relation2, …, relationm}
Remote supervised
{<entity1, entity2>} match
Sentence 1: …. entity1…. entity2….
Sentence 2: …. entity2…. entity1….
Sentence n: …. entity2…. entity1….
PREDICT: f({features})
{features}
Feature set
Words & POS in between 2 entities & PoS
■ Order of 2 entities
■ Words & POS of k words on left
■ Words & POS of k words on right
■ Entity Type
■ path between 2 entities in dependency tree
Dependency tree
Kiem-Hieu Nguyen. “BKTreebank: Building a Vietnamese dependency treebank". LREC 2018. http://45.117.171.213/bknlptool/
5. Coreference resolution
Coreferencing resolution is process of detecting a pair of words or phrases in text that refer to same entity
■ Coreferencing is a common phenomenon in languages
■ Coreferencing resolution is important for information extraction
Types of coreferences
Pronoun as subject: “Cô ta đang học trực tuyến"
■ Pronoun as object: “Hãy liên lạc với anh ấy ngay"
■ Possessive pronoun: “Lịch trình của chúng ta đã được thống nhất"
■ “Anh ta tự làm khó mình"
Types of coreferences (cont.)
First name: “Thủ tướng Nguyễn Xuân Phúc tuyên bố giãn cách xã hội. Thủ tướng Phúc cũng yêu cầu người dân tự giác thực hiện các quy định."
■ Apposition: “Phạm Nhật Vượng, Chủ tịch
Vingroup là một trong số các tỉ phú được Forbes nêu tên."
■ Verb 'là' : “Park Hang Seo là HLV trưởng đội tuyển bóng đá nam Việt Nam."
Types of coreferences (cont.)
Group people: “Mây Trắng tuyên bố tái hợp. Nhóm dự định ra mắt album mới đầu năm sau."
■ Attribute - value: “Giá cổ phiếu VIC là 94.800
VND"
■ Order: “IBM và Microsoft là những ứng cử viên cuối cùng, nhưng đại diện nhà đầu tư ưu tiên ứng cử viên thứ hai."
■ Part - whole: “Vinfast mới ra mắt dòng xe mới. Bộ truyền động sử dụng công nghệ CVT vô cấp tiên tiến."
Traditional methods
Focus on pronouns which are most common occurrences
■ Using linguistic information to spot frontrunners
■ Eliminate candidates based on properties such as gender, singular plural, etc.
■ Score candidates
Matching
■ Rule
■ Machine Learning
Neural network based method
Limit use of complex features
■ Limit use of parsers
■ Take advantage of pre-trained representation
■ Challenge:
Use alternative information for syntax information
■ Expressing phrases, contexts
■ Coreferencing resolution is essentially a hard clustering problem within text
Model architecture
Lee et al. “End-to-end Neural Coreference Resolution". EMNLP 2017.
Model architecture (cont.)
Problem statement
Document D consists of a sequence of words w1, w2,…, wT
■ D contains N = T(T+1)/2 paragraphs with length from
1 to T
■ paragraphs are sorted by position of starting word START(i); paragraphs with same starting word are sorted by position of ending word END(i)
■ With each paragraph i, find paragraph j preceding it representing an entity i refer to: j = yi if i not refer to any paragraph yi = ε
Input layer
Word Embedding:
Combined Glove 300 dim & Turian et al. (2010)
■ OOV: Vector 0
CNN-based character representation:
Input character has 8 dimensions
■ Windows {3, 4, 5} character, each with 50 filters
Character representation based on
CNN from Xuezhe Ma & Eduard Hovy. “End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF". ACL 2016
Contextual representation
 word representation is fed into 2 LSTM
Forward LSTM: Shows dependence of current word on previous words in sentence
■ Backward LSTM: Shows dependence of current word on following words in sentence
■ final representation is concatenation of 2 representations
Paragraph reprentation gi = [x∗START(i), x∗END(i), x̂i, Φ(i)]
■ x
START(i): First word representation
■ x
END(i): Last word representation
■ x̂ : "soft" representation of main word in paragraph is based on attention mechanism
■ Φ(i): Represents length of i ( number of words in i)
Soft representation of main word
FNNNα: feed forward neural network learn attention weights
■ w : Link weights of FNNN
■ α : Output of FNNN at time t
Scoring mention sm(i) = wm · FFNNm(gi)
■ g : Paragraph i representation
■ FNNN : feed forward neural network score mention
■ w : link weight of FNNN
Calculate similarity sa(i, j) = wa · FFNNa([gi, gj, gi ◦ gj, Φ(i, j)])
■ FNNN : feed forward neural network computes similarity between 2 segments i & j
■ w : link weight of FNNN
■ g ◦ g : inner product
■ Φ(i, j): Represents speaker information & gender & distance between 2 paragraphs i & j
Loss function
Marginal probabilities of segments representing entities
■ s(i, y ): Possibility i refer to y
Evaluation
Evaluation (cont.)
Lesson 8:
Recommender System
Agenda
1. Summary on Recommender System
2. Evaluation methods
3. Colaborative filtering using k-Nearest Neightbors
4. Colaborative filtering using Maxtrix Factorization
5. Neural Colaborative Filtering
6. Session-based Recommender System
1. Summary
Why we need recommender system?
Users are overloaded with information on web
Sellers need to offer right product to
Increase sale revenue
■ Improve serving quality
 trend of personalization & digitization is inevitable
Recommender System & Query System
Query System: Users express their wishes through a query
■ Recommender System : Users do not know what they want
Application fields
Ecommerce
■ Online entertainment
■ Online News
Forums, social networks
■ Scientific research
■ Online Dating
Application fields (cont.)
Amazon:
Netflix:
Recommend products
More than 30% increase in revenue
Recommend movies, TV shows
Bring in $1B per year
Google News:
News Suggestions
Nearly 40% increase in traffic
Application fields (cont.)
From Y. Koren
Recommendation methods
Content-based:
Colaborative filtering:
Suggestions based on users with similar interests
Session-based:
Suggestions based on user's transaction history
Suggestions based on transaction chains
Hybrid methods
Netflix challenge
Global average: 1.1296
Find better items erroneous
User average: 1.0651
Movie average: 1.0533
Personalization
Cinematch: 0.9514; baseline
“Algorithmics"
Static neighborhood: 0.9002
Static factorization: 0.8911
Time effects
Dynamic neighborhood: 0.8885
Dynamic factorization: 0.8794 accurate
Grand Prize: 0.8563; 10% improvement
Inherent noise: ????
From Y. Koren
Netflix challenge (cont.)
Number of papers on recsys by years
Content-based from Recommender Systems: An
Colaborative Filtering from Recommender Systems: An
Challenge of recommender system
 number of transactions is very small compared to actual number of users & products
■ Not enough information about new users & products
■ Users & products change over time, seasonally
Consumption habits change over time, seasonally
■ Real-time recommendation
2. Evaluation methods
Give
User set U
Item set I
Dataset of transactions (u, i, rui, t) u: user u ∈ U i: item I ∈ I rui: ratings of user u to item i t: rating time
■ rui
5-stage scale (1, 2, 3, ,4 ,5)
Binary scale (0, 1)
Dataset separated into train set & test set
■ RS is trained on train set
■ On test set, RS predict ratings p ui of user u to item i
train/test u1 i1 i2 u2 u3 i3 trai i1 i2 i3 i4 u1 u2 u3 i4 i1 u1 u2 i2 i3 i4 u3 tes
Metrics
(N)MAE
■ RMSE
■ Ranking:
Precision/Recall/F-score
MAE pui: prediction of model on ratings of user u to item i rui: ratings of user u to item i
■ n: number of examples in test set
NMAE rmax: User's maximum ratings value rmin: User's minimum ratings value
RMSE
A/B testing
100% users
100-x% users x% users
Recomme nder system
Original system
Evaluate
(E.g pageview,
CTR)
3. Colaborative Filtering using kNN
Based on product user interaction matrix
■ No training
■ Recommend based on users
Find set V of similar users to user u
Compute ratings for item i based on ratings of user in V to item i
Recommend based on users from B. Sarwar et al 2010
User similarity
C: set of item on that both user u & user v rated
■ r : Average ratings of user u (only counting for items that u rated)
Predict ratings
V: top k similar users to user u
Example (k = 2) i1 i2 i3 i4 u1 u2 u3 u4 r1 =
14/4 r2 = 3/2 r3 =
13/3 r4 = 8/3 sim(u3,u1) =
~0.492 sim(u3,u2) =
~0.948 sim(u3,u4) =
~0.919 p(u3,i4) = ~ 6.67
Disadvantages
Regularly update user vector when user has a new transaction
■ Requires computation on entire set of users
Recommend products
Product representation based on user-product interaction matrix
■ Suitable for systems with product number << number of users
■ Lower update frequency of product vectors
■ Can calculate product-product similarity in advance
Product representation from B. Sarwar et al 2010
Product similarity
U: set of users rated both item i & item j
Ratings prediction
J: top k similar items to item i
Example from B. Sarwar et al 2010
4. Colaborative Filtering using MF
Based on overall context of user-product interaction
■ Representing users, products in latent aspects
■ Using Matrix Factorization technique
Based on user & product vectors to make predictions
SVD
User – movies matrix R
■ Factorize R into user-aspect matrix U & moviesaspect matrix M
■ Each user is represented by a K-dimensional vector, where K is number of latent aspect
■ Each movie is represented by a K-dimensional vector
Ratings predictions pij: predict ratings of user i to movie j
■ u : User vecrtor i
■ m : movie vector j
Hàm lỗi: eij2
Model
U, M is learning parameters
■ R is training dataset
2 = (p
■ Mean Square Error: E = e ij ij ij – ij
Learning technique: Gradient Descent
Gradient Descent
Derivative of error function wrt uki
Gradient Descent (Cont.)
Derivative of error function wrt mkj
Gradient Descent (cont.)
Update uki:
Update mkj : λ: regularize parameter γ: learning rate
5. Neural Colaborative Filtering
Representing user, product in a hidden aspect level may not fully capture complex nature of userproduct interaction
■ Deep neural networks allow automatic learning of latent aspect levels from basic to abstract
■ MF is basic form of neural network
Limitations of MF
MF preserve similarity of user vectors
■ Assume that user similarity is measured using Jaccard metric
■ User vector & item vector are both represented in 
K-dimensional latent aspect space
Limitations MF (cont.) s23 (0.66) > s12 (0.5) > s13 (0.4)
■ s
41 (0.6) > s43 (0.4) > s42 (0.2) from He et al. “Neural Collaborative Filtering". WWW 2017.
NCF architecture
Input layer
 user is represented by a M-dimension 1-hot vector
M is number of users
Item are represented by a N-dimension 1-hot vector where N is number of products
Embedding layer
Independently represent users & products
■ M x K connection weight to represent user
■ N x K connection weights to represent item
E.g: K = 16
MLP
User & item representations are feeded into a multilayered MLP network to learn complex user & product interactions
■ E.g:
ReLU activation function
3 layers with deceasing size 32 → 16 → 8
Loss function
Implicit feedback:
1: user interacts with item
0: user do not interacts with item
Binary classification problem with probability factor
■ Using cross entropy loss function
6. Session-based
In many cases, it is difficult to identify users & collect reviews.
Small commercial websites
News sites session-based recommendation
Do not require user identification
Each transaction, including transaction order, is used as model training data
Problem definition
Input: Stream of items i1, i2, …, in-1
■ Output: suggest next item
■ Given a list of products with highest probability Pr(i | i1, i2, …, in-1)
Suitable for implicit feedback
Implicit feedback from RENDLE ET AL. “BPR: Bayesian Personalized Ranking from Implicit Feedback". 2009
RNN architecture item 2 item item item
... item n item 1 item item item
... item (n1)
Outpu t layer
Hidden layer
Input layer
Input layer
Represent each item at a time t as V-dimension 1-hot vector
1 at position of item, otherwise 0
V: number of distinct
Embedding layer
Biến đổi biểu diễn 1-hot thành biểu diễn K chiều
K number of neural in embedding layer
Number of connection weight between input & embedding layer V x K
Recurrent layer
 recurrent layer stores historical information through recurrent links
■ Many recurrent layer stacked together to learn high level abstract features
■ Advanced model: LSTM or GRU
MLP layer
 output of recurrent layer is used as input of MLP layer to generate prediction
■ MLP layer can include hidden layers to learn nonlinear function
■ MLP has V neurons corresponding to V products
Loss function
Pair-wise rank loss function
N: number of negative sample
■ r̂ : ratings of positive sample i
■ r̂ : ratings of negative sample j
Implicit feedback hypothesis: item i is selected so i's priority is higher than any other item j
Lesson 9:
Online Ad &
Query Mining
Agenda
1. Online advertising
2. Search engine advertising
3. Query Mining
Ads
Content
User
Content
Provider from Agarwal, D
Pick ads
Ad
Network
Advertisers
1. Online Advertising
Examples:
Yahoo, Google,
MSN, RightMedia,
Online advertising model
Online
Advertising
Revenue
Models
CPM
CPC
CPA
Display
Misc.
Ad exchanges
Advertising
Setting
Content
Match
Sponsored
Search
CPM
CPC
CPA
Cost Per iMpression
Ads
Ad
Network
Pick ads
Content
User
Content
Provider
Advertisers
CPM
CPC
CPC
CPA
Cost Per
Click click
Ads
Ad
Network
Pick ads
Content
User
Content
Provider
Advertisers
CPM
CPA
CPC
CPA
Ad
Network click
Ads
Pick ads
Content
User
Content
Provider
Advertisers
CPM
Cost Per
Action
Revenue - CPM
CPM CPC CPA
Assume that an ad is shown N items at same position
CPM: Revenue = N * CPM
Revenue - CPC
CPM CPC CPA
Assume that an ad is shown N items at same position
CPM: Revenue = N * CPM
Depends on
CPC: Revenue = N * CTR * CPC auction mechanism
Click-through Rate
(probability of clicking on an ad)
Revenue - CPA
CPM
CPC
CPA
Assume that an ad is shown N items at same position
CPM: Revenue = N * CPM
CPC: Revenue = N * CTR * CPC
CPA: Revenue = N * CTR * Conv. Rate * CPA
Conversion Rate
( probability that user takes an action when viewing ad page)
2. Search engine advertising
Query
Paid Ad
Search engine advertising model
Display
Content
Match
Sponsored
Search
Text ads
Search
Query
Pick ads
Match ads to query
Maximize revenue
 problem of advertising company
Select ads for maximum revenue
Match query
■ Advertising costs
■ Ad page quality
Scoring based on content
Consider advertising like a text
■ Compare query similarity to ads
■ Methods
Vector space model
Language model
Language model
QUERY
LM
AD
LM
P(ad|query
LM)
Language model (cont.)
QUERY
LM
AD
LM
P(query|ad
LM)
Language model (cont.)
QUERY
LM
AD
LM
KL(ad LM;query
LM)
Pros & Cons
Pros
Simple model
■ Suitable for short popular query
Cons:
Hardly handle rare queries (long tail)
■ Hardly process in real-time
■ Not using user feedback
Score based on user feedback
Query set Q
■ Ad page set A
■ For each query q ∈ Q & ad page a ∈ A, compute probability that user clicks on ad page Pr(click| q, a)
Using user feedback to estimate probabilities
Logistic Regression
Representation of query & advertising content in vectors (bag of words)
■ Pr(click| q, a) = f(q, a; θ)
■ Logistic Regression:
Log-odds (Pr(click |q, a)) = q' W a
Estimate W using user feedback as training data from Wikipedia
Collaborative filtering
Interactive matrix query, advertising
■ Use latent user feedback (click on ad page)
■ For each query q & ad page a, predict user interest
Collaborative filtering
Using kNN
Represent ads by query to calculate similarity
Collaborative filtering (cont.)
Top similar ad to ad a
Relevant level of ad a to query q
Similarity matrix ad-ad
3. Query Mining
Google: 40,000 query/s
Query features
A query contains an average of 2.4 words
21% of internet traffic comes from search engines
User feedback
50% click on first result
■ Users mostly only use first 2 results
Query features (cont.)
Users often edit query
Search trends shift from entertainment to ecommerce, in which product search accounts for 1/5
 distribution of vocabulary on query & on website content is different → what users search for is different from what is available on internet
Query logging
User information
Query content
List of relevant documents
Selected documents of user
Query preprocessing
Identify query session
■ Filter bot query
■ Standardize query
Identify query session
Classify pairs of consecutive queries into classes :
Same query content but different search scope
■ Query Generalization
■ Query fine-tuning for a more precise query
■ Query detailing
■ New query content
Filter bot query
Query generated by bot to collect search engine results
Duplicate content
Unusually high query rate &/& recurring query frequency
Standardize query
Remove stopwords
Convert to lower case
Standardize number
Stemming
For Vietnamese
Restore accent
■ Tokenize
Application 1: Query suggestion
Language model
Learn language model on query data argmaxw P(w|w0,w1,...wn-1,wn)
Require large query dataset
■ basic unit of language model word (tokenize) syllable demisyllabel ('ch', 'ang')
Character
n-gram language model
Unigram
P(w) = (count(w)+1) / (sumw' count(w')+V)
Bigram
P(w0,w1) = P(w1|w0)*P(w0)
P(w1|w0) = (count(w0,w1)+1) / (sumw' count(w0,w')+V)
Application 2: Extend query
User queries often do not contain enough information
■ Query expansion based solely on textual content may not meet user needs properly
Using user feedback
Assumption: If a query containing 1 keyword leads to related documents containing another keyword, it is likely that 2 keywords are related.
Extend query model from Hang Cui et al
Extend query model (cont.)
P(wj(d) | Dk) : probability of wj(d) given selected Dk
P(Dk | wi(q)) : probability of Dk to be selected if wi(q) appears in query
Extend query model (cont.) fik(q)(wi(q), Dk) : number query session in which query contain wi(q) & Dk is seleted f(q)(wi(q)) : number of query session in which query contain wi(q)
Wjk(d) : Weight of wj(d) in document Dk
Extend query model (cont.)
1. Extract term in query Q
2. Find documents related to any term
For each term in each document, use formula to measure relevance to query Q
4. Using top n highest score term to construct query
Q'
5. Search with query Q'
Application 3: Disease warning https://www.google.org/flutrends
Based on related queries
 number of people looking for information about disease is proportional to number of people who are sick
Jeremy Ginsberg et al
